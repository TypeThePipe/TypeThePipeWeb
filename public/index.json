[{"authors":["pablo-canovas"],"categories":null,"content":"Pablo Cánovas is a data scientist with experience developing machine learning models applied to electricity markets.\n","date":1589328000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1589328000,"objectID":"78b4973aee7e471f2427600fe9f47d84","permalink":"/authors/pablo-canovas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/pablo-canovas/","section":"authors","summary":"Pablo Cánovas is a data scientist with experience developing machine learning models applied to electricity markets.","tags":null,"title":"Pablo Cánovas","type":"authors"},{"authors":["carlos-vecina"],"categories":null,"content":"Carlos Vecina is a data scientist with experience using ML and AI to bring value to business in CRM, Marketing and energy markets environments.\n","date":1585699200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1585699200,"objectID":"7f2ce30e6e7155580a08c73da392044a","permalink":"/authors/carlos-vecina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/carlos-vecina/","section":"authors","summary":"Carlos Vecina is a data scientist with experience using ML and AI to bring value to business in CRM, Marketing and energy markets environments.","tags":null,"title":"Carlos Vecina","type":"authors"},{"authors":["Pablo Cánovas"],"categories":["R","Tidyverse","Tips"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rCalculating percentages is a fairly common operation, right?\rHowever, doing it without leaving the pipeflow always force me to do some bizarre piping such as double grouping and summarise.\nI am using again the nuclear accidents dataset, and trying to calculate the percentage of accidents that happened in Europe each year.\ndiv.sourceCode {\roverflow-x: hidden;\r}\r\rnuclear_accidents \u0026lt;-read_csv(\u0026quot;https://query.data.world/s/47s7katrhxxd674ulus425k42l5u4s\u0026quot;) \r\rnuclear_accidents \u0026lt;-nuclear_accidents %\u0026gt;%\rselect(-Description) %\u0026gt;%\rmutate(Year = Date %\u0026gt;%mdy() %\u0026gt;%year(),\rIn_Europe = if_else(Region %in%c(\u0026quot;EE\u0026quot;, \u0026quot;WE\u0026quot;), T, F) %\u0026gt;%as.factor()) %\u0026gt;%\rfilter(Year %\u0026gt;%between(1989, 2016))\r\rnuclear_accidents %\u0026gt;%head(4) %\u0026gt;%kable()\r\r\r\rDate\r\rLocation\r\rCost (millions 2013US$)\r\rINES\r\rSmyth Magnitude\r\rRegion\r\rFatalities\r\rYear\r\rIn_Europe\r\r\r\r\r\r3/11/2011\r\rFukushima Prefecture, Japan\r\r166089.0\r\r7\r\r7.5\r\rA\r\r573\r\r2011\r\rFALSE\r\r\r\r12/8/1995\r\rTsuruga, Japan\r\r15500.0\r\rNA\r\rNA\r\rA\r\r0\r\r1995\r\rFALSE\r\r\r\r12/19/1989\r\rVandellòs, Spain\r\r930.9\r\r3\r\rNA\r\rWE\r\r0\r\r1989\r\rTRUE\r\r\r\r2/1/2010\r\rVernon, Vermont, United States\r\r808.9\r\rNA\r\rNA\r\rNA\r\r0\r\r2010\r\rFALSE\r\r\r\r\rThis can be achieved by several ways. One common path would be:\nnuclear_accidents %\u0026gt;%\rgroup_by(Year, In_Europe) %\u0026gt;%\rsummarise(N = n()) %\u0026gt;%\rgroup_by(Year) %\u0026gt;%\rmutate(Total_per_year = sum(N), \rRatio = round(N/Total_per_year, 2)) %\u0026gt;%\rhead(4) %\u0026gt;%\rkable()\r\r\r\rYear\r\rIn_Europe\r\rN\r\rTotal_per_year\r\rRatio\r\r\r\r\r\r1989\r\rFALSE\r\r4\r\r6\r\r0.67\r\r\r\r1989\r\rTRUE\r\r2\r\r6\r\r0.33\r\r\r\r1990\r\rFALSE\r\r1\r\r1\r\r1.00\r\r\r\r1991\r\rFALSE\r\r3\r\r3\r\r1.00\r\r\r\r\rAnother one more bizarre would be totalizing first, then grouping including that amount (to avoid being dropped) and then summarise.\nnuclear_accidents %\u0026gt;%\rgroup_by(Year) %\u0026gt;%\rmutate(Total_per_year = n()) %\u0026gt;%\rgroup_by(Year, In_Europe, Total_per_year) %\u0026gt;%\rsummarise(N = n()) %\u0026gt;%\rmutate(Ratio = round(N/Total_per_year, 2)) %\u0026gt;%\rhead(4) %\u0026gt;%\rkable()\r\r\r\rYear\r\rIn_Europe\r\rTotal_per_year\r\rN\r\rRatio\r\r\r\r\r\r1989\r\rFALSE\r\r6\r\r4\r\r0.67\r\r\r\r1989\r\rTRUE\r\r6\r\r2\r\r0.33\r\r\r\r1990\r\rFALSE\r\r1\r\r1\r\r1.00\r\r\r\r1991\r\rFALSE\r\r3\r\r3\r\r1.00\r\r\r\r\rKind of weird.\rHowever, there is a much simpler way:\nnuclear_accidents %\u0026gt;%\rgroup_by(Year, In_Europe) %\u0026gt;%\rsummarize(N = n()) %\u0026gt;%\rmutate(Ratio = round(N /sum(N), 2)) %\u0026gt;%\rhead(4) %\u0026gt;%\rkable()\r\r\r\rYear\r\rIn_Europe\r\rN\r\rRatio\r\r\r\r\r\r1989\r\rFALSE\r\r4\r\r0.67\r\r\r\r1989\r\rTRUE\r\r2\r\r0.33\r\r\r\r1990\r\rFALSE\r\r1\r\r1.00\r\r\r\r1991\r\rFALSE\r\r3\r\r1.00\r\r\r\r\rThe first time I saw this result I didn’t understand it because if you have your dataframe grouped by Year and In_Europe then sum(N) should be equal to N.\rWhat is going on?\rThis behaviour has to do with a tricky funcionality of summarise.\nTake a closer look of the grouping variables at the console output.\rBefore the summarise function the dataframe seems grouped normally and the operation will be performed within each group:\nnuclear_accidents %\u0026gt;%\rgroup_by(Year, In_Europe) %\u0026gt;%\rhead(4)\r# A tibble: 4 x 9\r# Groups: Year, In_Europe [4]\r# Date Location `Cost (millions ~ INES `Smyth Magnitud~ Region Fatalities Year In_Europe\r# \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \r# 1 3/11/~ Fukushima ~ 166089 7 7.5 A 573 2011 FALSE \r# 2 12/8/~ Tsuruga, J~ 15500 NA NA A 0 1995 FALSE \r# 3 12/19~ Vandellòs,~ 931. 3 NA WE 0 1989 TRUE \r# 4 2/1/2~ Vernon, Ve~ 809. NA NA NA 0 2010 FALSE \rHowever, once the dataframe is summarized, the resulting dataframe is no longer grouped by the same original variables:\nnuclear_accidents %\u0026gt;%\rgroup_by(Year, In_Europe) %\u0026gt;%\rsummarize(N = n()) %\u0026gt;%\rhead(4)\r# A tibble: 4 x 3\r# Groups: Year [3]\r# Year In_Europe N\r# \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r# 1 1989 FALSE 4\r# 2 1989 TRUE 2\r# 3 1990 FALSE 1\r# 4 1991 FALSE 3\rActually, the default behaviour of summarise is to drop the last group. The reason behind that is that, once the operation is performed you should have only one obervation per group, and it has no sense to grouping by it anymore.\rThat’s why the last example I show above works.\rNow you can take advantage of it too!\nFurthermore, you can learn more about the dplyr 1.0.0 last minute additions which include an explicit message to highlight the behaviour we have talked about here.\n","date":1589328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589328000,"objectID":"71c41809596b34fe80ddbb06074e6d36","permalink":"/vizs-and-tips/calculating-ratios-with-tidyverse/","publishdate":"2020-05-13T00:00:00Z","relpermalink":"/vizs-and-tips/calculating-ratios-with-tidyverse/","section":"vizs-and-tips","summary":"Explaining summarise hidden behaviour","tags":[],"title":"Calculating ratios with Tidyverse","type":"vizs-and-tips"},{"authors":["Pablo Cánovas"],"categories":["R","Tidyverse","Tips"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rdiv.sourceCode {\roverflow-x: hidden;\r}\r\rThis week I learned about another neat trick with tidyverse functions: the argument .drop from the group_by function.\nTo showcase this functionality I made up a simple example with this dataset consisting of nuclear accidents data.\noriginal_data \u0026lt;-read_csv(\u0026quot;https://query.data.world/s/47s7katrhxxd674ulus425k42l5u4s\u0026quot;)\rTake a sneak peek of the dataset:\n\r\r\rDate\r\rLocation\r\rCost (millions 2013US$)\r\rINES\r\rSmyth Magnitude\r\rRegion\r\rFatalities\r\r\r\r\r\r4/26/1986\r\rKiev, Ukraine\r\r259336\r\r7\r\r8.0\r\rEE\r\r4056\r\r\r\r3/11/2011\r\rFukushima Prefecture, Japan\r\r166089\r\r7\r\r7.5\r\rA\r\r573\r\r\r\r12/8/1995\r\rTsuruga, Japan\r\r15500\r\rNA\r\rNA\r\rA\r\r0\r\r\r\r3/28/1979\r\rMiddletown, Pennsylvania, United States\r\r10910\r\r5\r\r7.9\r\rNA\r\r0\r\r\r\r\rLet’s say we are interested in knowing about how does the percentage of accidents happened in Europe vary compared to the rest of the world, and for that reason we simplify the Region variable onto the In_Europe boolean feature:\ndf \u0026lt;-original_data %\u0026gt;%\rmutate(Year = Date %\u0026gt;%mdy() %\u0026gt;%year(),\rIn_Europe = if_else(Region %in%c(\u0026quot;EE\u0026quot;, \u0026quot;WE\u0026quot;), T, F) %\u0026gt;%as.factor()) %\u0026gt;%\rfilter(Year %\u0026gt;%between(1989, 2011))\rWe then simply compute the percentage of accidents happened every year and plot it\ndf_implicitNAs \u0026lt;-df %\u0026gt;%\rgroup_by(Year, In_Europe) %\u0026gt;%\rsummarize(N = n()) %\u0026gt;%\rmutate(Percentage = round(N /sum(N) *100, 1)) \rIf that chunk puzzles you, I explain what is going on under the hood in this post.\nIn the plot below I highlighted the strange result I found: being that there are only two possibilities (the accident happened in Europe or not) the sum of the ratios should add up to 100, right?\rNot a beautiful plot but aesthetic considerations apart, what is going on here? Did we fail with the maths?\rNot exactly. This result is due to a tricky behaviour of the summarise function related with the missing values.\nIf we take a look to the first rows, we can see how in 1990 and 1991 there weren’t any nuclear accident in Europe, but that information is implicit instead of explicit.\n\r\r\rYear\r\rIn_Europe\r\rN\r\rPercentage\r\r\r\r\r\r1989\r\rFALSE\r\r4\r\r66.7\r\r\r\r1989\r\rTRUE\r\r2\r\r33.3\r\r\r\r1990\r\rFALSE\r\r1\r\r100.0\r\r\r\r1991\r\rFALSE\r\r3\r\r100.0\r\r\r\r1992\r\rFALSE\r\r3\r\r75.0\r\r\r\r1992\r\rTRUE\r\r1\r\r25.0\r\r\r\r\rWhen plotting with the line graph, ggplot is connecting the data points between 1989 and 1992 and therefore displaying misleading information.\rObviously this is not ggplot’s fault, it’s simply how it works.\nWe could easily solve this problem chossing an more suitable graph as we will see below, but this example is still useful to learn about the .drop argument:\rThe default behaviour of the group_by function is to drop zero-length groups, and therefore it’s making implicit that piece of information. We can override the default behaviour simply adding .drop = FALSE to the call:\ndf_explicitNAs \u0026lt;-df %\u0026gt;%\rgroup_by(Year, In_Europe, .drop = FALSE) %\u0026gt;%\rsummarize(N = n()) %\u0026gt;%\rmutate(Percentage = round(N /sum(N) *100, 1))\rNow we can see the years when there were no accidents in Europe:\n\r\r\rYear\r\rIn_Europe\r\rN\r\rPercentage\r\r\r\r\r\r1989\r\rFALSE\r\r4\r\r66.7\r\r\r\r1989\r\rTRUE\r\r2\r\r33.3\r\r\r\r1990\r\rFALSE\r\r1\r\r100.0\r\r\r\r1990\r\rTRUE\r\r0\r\r0.0\r\r\r\r1991\r\rFALSE\r\r3\r\r100.0\r\r\r\r1991\r\rTRUE\r\r0\r\r0.0\r\r\r\r\rAnd now the graph doesn’t deceive us anymore:\nLet’s face, however, that we could have tackled this problem choosing a more suitable geom. Whenever you need to display any normalized variable, using geom_col() is usually a better approach:\n","date":1588982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588982400,"objectID":"853a501dcf18003a19bc5d7babb19abd","permalink":"/vizs-and-tips/preserving-zero-length-groups/","publishdate":"2020-05-09T00:00:00Z","relpermalink":"/vizs-and-tips/preserving-zero-length-groups/","section":"vizs-and-tips","summary":"Don't drop that","tags":[],"title":"Preserving zero-length groups","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["R","Tips"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rYesterday I was playing with the new and first Thomas Neitman’s CRAN package!\nIts documentation reminded me that more than a year ago Type The Pipe did a talk series about text mining. In one of them, we analized the reviews of the most important spanish banking APPs.\nOne of the first plots was a lollipop chart showing the ratings in the Play Store, and its deviation from the mean. I used to remember this plot cooler than it actually seems to me lol\rIt’s maybe telling us that we have improved our skills. Hope it.\nlibrary(tidyverse)\rlibrary(purrr)\r\r# Data Prep\rreviews_score \u0026lt;-fst::read_fst(\u0026quot;reviews_score.fst\u0026quot;)\rglobalMean \u0026lt;-round(mean(reviews_score$Mean),2)\rcenter_formatter \u0026lt;-function(x) {x +globalMean} # Set the mean as a center\rcustom_labs \u0026lt;-labs(title=\u0026quot;Ranking Spanish Banking APPs reviews (1 to 5 stars) \u0026quot;, \rsubtitle=paste0(\u0026quot;Filtering out reviews without comment. Last week data. The mean score is: \u0026quot;, globalMean), \rx=\u0026quot;\u0026quot;,\ry=\u0026quot;Score\u0026quot;)\r\r\rggplot(reviews_score, aes(x=Bank, y=MeanDev, label=Mean)) +\rgeom_segment(aes(x = Bank, y = 0,\ryend = MeanDev, xend = Bank,\rcol=AboveBelow),size=2)+\rgeom_point(stat=\u0026#39;identity\u0026#39;, aes(col=AboveBelow), size = 14) +\rguides(colour = guide_legend(override.aes = list(size=2))) +\rscale_color_manual(name=\u0026quot;Scoring\u0026quot;, \rlabels = c(\u0026quot;Above\u0026quot;=\u0026quot;Above the mean\u0026quot;,\r\u0026quot;Below\u0026quot;=\u0026quot;Below the mean\u0026quot;), \rvalues = c(\u0026quot;Above\u0026quot;=\u0026quot;#00ba38\u0026quot;,\r\u0026quot;Below\u0026quot;=\u0026quot;#f8766d\u0026quot;)) +\rgeom_text(color=\u0026quot;black\u0026quot;, size=5) +\rcoord_flip() +\rtheme_minimal() +\rscale_y_continuous(labels=center_formatter) +\rcustom_labs\rWell, it’s quite good if you don’t pay attention to the big circles or to the long code to write such an “standard” chart.\nHere is where ggcharts shines. And it’s only the 0.1.0 version. But having reusable plot templates in a quite flexible way seems such a cool idea for me.\nSee this three-liner approach to the above plot:\n# install.packages(\u0026quot;ggcharts\u0026quot;) :)\rlibrary(ggcharts)\r\rlollipop_chart(reviews_score, x=Bank, y=MeanDev, color=AboveBelow) +\rscale_color_manual(name=\u0026quot;Scoring\u0026quot;, \rlabels = c(\u0026quot;Above\u0026quot;=\u0026quot;Above the mean\u0026quot;,\u0026quot;Below\u0026quot;=\u0026quot;Below the mean\u0026quot;), \rvalues = c(\u0026quot;Above\u0026quot;=\u0026quot;#00ba38\u0026quot;, \u0026quot;Below\u0026quot;=\u0026quot;#f8766d\u0026quot;)) +\rscale_y_continuous(labels=center_formatter) +custom_labs \rThat was so cool and quite much cleaner.\nThere are several paths to keep improving its capabilities. So here are some possible enhancements. Ask him for yours… see you in his ggcharts repo!\n\nProposed enhancements to R ggcharts:\n\rEasy \u0026amp; Quick auto-aligned Annotations:\r\rreviews_score %\u0026gt;%\rlollipop_chart(x=Bank, y=MeanDev, limit = 25, #line_color = colours,\rhighlight = \u0026quot;BBVA\u0026quot;,point_color = \u0026quot;blue\u0026quot;) +\rscale_y_continuous(labels=center_formatter) +\rannotate(\u0026quot;segment\u0026quot;, x = 5, xend = 5.3, y = 0.55, yend = 0.7, colour = \u0026quot;black\u0026quot;) +\rannotate(\u0026quot;text\u0026quot;, x = 5.4, y = 0.75, label = \u0026quot;4.05\u0026quot;) +custom_labs\r\rEasy Image axis and plot insertion:\r\r# https://gist.github.com/jonocarroll/2f9490f1f5e7c82ef8b791a4b91fc9ca\r# https://stackoverflow.com/questions/54973129/including-images-on-axis-label-in-an-animated-ggplot2\r\rNon 0 centered in lolliplot and diverging lolliplot:\r\r# center parameter?\rscale_y_continuous(labels = function(x) {x +center})\r\rColor aes and highlight conflict\r\r# Counter intuitive line_color parameter when highlight\rcolours \u0026lt;-c(\u0026quot;green, green\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;red\u0026quot;, \u0026quot;red\u0026quot;, \u0026quot;red\u0026quot;)\rlollipop_chart(reviews_score, x=Bank, y=MeanDev, limit = 25, line_color = colours) # BBVA is green\rlollipop_chart(reviews_score, x=Bank, y=MeanDev, limit = 25, line_color = colours,\rhighlight = \u0026quot;BBVA\u0026quot;) # the highlighting BBVA now red\r\rdiverging_lollipop_chart(reviews_score, Bank, Mean) # not working with all positive values (linked with to center in non 0 value)\r\nEnjoy ggcharts!\n\n\rwindow.dojoRequire([\"mojo/signup-forms/Loader\"], function(L) { L.start({\"baseUrl\":\"mc.us4.list-manage.com\",\"uuid\":\"91551f7ed29389a0de4f47665\",\"lid\":\"d95c503a48\",\"uniqueMethods\":true}) })\r","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"0d6ba338eefda9f0872d393ed0c8a6ce","permalink":"/vizs-and-tips/ggcharts-lollipop-plot-in-r/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/vizs-and-tips/ggcharts-lollipop-plot-in-r/","section":"vizs-and-tips","summary":"Thomas Neitman's ggcharts","tags":[],"title":"Playing with a new R package; Welcome ggcharts!","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["R","Tips"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rAre you trying to save and load your DL model or a big dataset in R? Here we show you a performance boost to your scripts and reduction in disk memory storage with the FST CRAN package. We are going to benchmark it with R base functions (csv and RDS extensions) and another great package like readr:\nlibrary(tidyverse)\rbig_dataset %\u0026gt;%nrow() # 700k rows, 15 cols(8 factor, 4int, 3 logi)\rlibrary(microbenchmark)\rlibrary(readr)\rlibrary(fst)\r\rmicrobenchmark(\rwrite.csv(big_dataset, paste0(path,\u0026quot;big_dataset.csv\u0026quot;),), # utils\rwrite_csv(big_dataset, paste0(path,\u0026quot;big_dataset.csv\u0026quot;)), # readr\rwrite_csv(big_dataset, paste0(path,\u0026quot;big_dataset.csv.gz\u0026quot;),), # readr GZ\rsaveRDS(big_dataset, paste0(path,\u0026quot;big_dataset.RDS\u0026quot;)), # utils\rwrite_rds(big_dataset, paste0(path,\u0026quot;big_dataset.RDS\u0026quot;)), # readr\rwrite_fst(big_dataset, paste0(path,\u0026quot;big_dataset.fst\u0026quot;)), # fst\rtimes = 10\r)\r## Unit: milliseconds\r## min mean median max neval file_size\r##utils 10943.1161 11232.20073 11098.66610 12011.1538 10 109 MB\r##readr 3140.4450 3442.92772 3388.14280 3768.4109 10 109 MB\r##readrGZ 6993.8850 7332.31976 7260.95040 7946.9233 10 23 MB\r##base 4800.3516 5122.22345 5024.69395 5833.9807 10 15 MB\r##readr 187.0765 210.74584 211.70760 246.6369 10 46 MB\r\u0026quot;fst 60.3065 87.30611 74.94375 154.7718 10 16 MB\u0026quot;\r\nWow! That was cool! We can achieve an amazing reading and writing speed plus an incredible file size!\nWe can see a x3 and x50 performance improvements over the readr::write_rds() and base saveRDS() functions!\nAn incredible x100 performance between fst and csv writing functions, but the true here is that they are not directly comparable as they work with quite different file formats.\n\nAre you going to add FST to your R projects toolbox too?\n\nSee related useful tips on TypeThePipe\n\n\rwindow.dojoRequire([\"mojo/signup-forms/Loader\"], function(L) { L.start({\"baseUrl\":\"mc.us4.list-manage.com\",\"uuid\":\"91551f7ed29389a0de4f47665\",\"lid\":\"d95c503a48\",\"uniqueMethods\":true}) })\r","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585526400,"objectID":"0ff90810143f0dd4c66ca71c7a35c5d3","permalink":"/vizs-and-tips/speed-up-load-write-files-fst-package-r/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/vizs-and-tips/speed-up-load-write-files-fst-package-r/","section":"vizs-and-tips","summary":"Unbeaten speed and file size! It's FST! x100 faster than write.csv()","tags":[],"title":"Speed up your R scripts. A cool optimized way to load, write and store big data frames with FST package!","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["R","Tips"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rAre you developing an automated exploration tool? Here we propose some alternatives to drop columns with high percentage of NAs.\nIn this previous tip we talk about BaseR vs Tidy \u0026amp; Purrr counting NAs performance.\nNot leaving the pipeflow. How much does it cost?;) It depends on the NA distribution between features and its number, but not much that a few nanoseconds in small and big datasets\n# library(microbenchmark) You can benchmark them in small and big datasets\rlibrary(tidyverse)\r\rairquality %\u0026gt;%select_if(~mean(is.na(.)) \u0026lt;0.2)\r\rairquality %\u0026gt;%select(which(colMeans(is.na(.)) \u0026lt;0.2))\r\rairquality[lapply(airquality, function(x) mean(is.na(x))) \u0026lt;0.2]\r\nSoooo what’s your choice??\n\n\rwindow.dojoRequire([\"mojo/signup-forms/Loader\"], function(L) { L.start({\"baseUrl\":\"mc.us4.list-manage.com\",\"uuid\":\"91551f7ed29389a0de4f47665\",\"lid\":\"d95c503a48\",\"uniqueMethods\":true}) })\r","date":1584921600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584921600,"objectID":"718207551b31bdb41be4b54104d49fcb","permalink":"/vizs-and-tips/selecting-columns-based-on-nas-percentage-r-tidyverse/","publishdate":"2020-03-23T00:00:00Z","relpermalink":"/vizs-and-tips/selecting-columns-based-on-nas-percentage-r-tidyverse/","section":"vizs-and-tips","summary":"Showing up several ways","tags":[],"title":"Drop columns based on NAs percentage in R","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["Post"],"content":"\r\r1. Set your main goal and check your time and resources (skills, hardware…)\r2. Choose the challenge field/industry according your interests.\r3. Build the right team. Set up tools to exchange code.\r4. Deep research about the challenge topic and environment.\r5. Excel your exploratory analysis. Remember you could use external data sources.\r6. Set your project and code structure.\r7. Run your models.\r8. Cross Validation, OOS and backtesting.\r9. Model interpretability\r10. Write down your observed strengths and weaknesses. It will be the starting point for your next proyect. \r\r\rAre you considering join a Datathon or data competition? In this post we’ll bring you 10 tips that can help you to outperform your competitors. So without further ado;\n\n1. Set your main goal and check your time and resources (skills, hardware…)\rDepending on your profile and the competition requirements, you should think about your main goal by joining a data challenge. It could be learning about a new tool, improve your coding and algorithmical skills, achieve the 1st position and therefore the prize, or just for fun. Whatever it is, that’s great.\nIt’s important to have a bright and well-defined picture of your goal, because you are going to invest so much time in the project. Having clear your personal idea of success will help you in your valley of despair.\n\n\r2. Choose the challenge field/industry according your interests.\rFollowing with the ‘you’re going to invest so much time’ idea, how are you joining a challange in a field that doesn’t inspire you? Hopefully, many organizations and platforms are launching it’s own data challanges open to the public. Kaggle is the principal one, nevertheless there are many more. We propose several options here.\n\n\r3. Build the right team. Set up tools to exchange code.\rYou should choose your teammates depending on your principal objective. It’s an unseen topic with related posts, but we consider it is sufficiently important one.\nYou better choose an inspiring teammate you feel good with and encourage you if you want to learn about a new topic or technology. If the project needs different profiles and you want to be among the prize winners, go for a multidisciplinary team. If the project doesn’t need it, choose a teammate with your skill level. If it’s possible, a little more skilled.\nThese are just a few examples. The main point here is choosing teammates to maximize invested time return by minimizing interpersonal problems and strengthen synergies.\n\n\r4. Deep research about the challenge topic and environment.\rOnce you have built the team, we propose you to start a research about the industry your challenge is on. This effort will lead you to a better understanding of the problem and the solution, avoiding useless iterations in your data science workflow. By starting with clear premises you will take apart basic concetps missunderstandings, a fact that could fool your whole solution and conclusions.\nLet’s explain this concept. Suppose your challenge consist on forecasting the conversion rate given a product portfolio, based on the online activity recorded by Google Analytics. Here, we must have clear-minded and relevant knowledge about this data source behaviour. How the bounce rate works, the customer journey recording since he open our web page and is matched with a cookie to the final conversion, leaving the web or logging as a user. Also, different behaviours of null records, bots, several cases where the default source is ‘direct’…\nWithout this kind of information it could be difficult to craft meaningful variables to increase the model performance. But worst of all, any conclusion you get is most likely to be misunderstood and impossible to get powerful and business-disruption insights.\n\n\r5. Excel your exploratory analysis. Remember you could use external data sources.\rAs any data science project, it consist of iterative phases. Since you have an industry landscape understanding, you will take a look on the data. If any doubt comes to your mind, return to the research stage.\nSo in this exploratory phase, you are focused on your data and extract information about features itself and interactions. We usually start summarizing the features by its distribution, number of NAs, categories… Features with high NAs percentage or with a minimal variance we can choose between removing or encoding it as binary variable NA/Not_NA Majoritay_Class/Not_Majoritary class. Many other encoding techniques can be applied, like these.. The full exploratory toolbox can be found on Google or Kaggle.\nFinally, as a reminder, you usually can use external data sources to enrich the provided data. Demographic data to contextualize your zip code feature. Or a past event to explain a spike in your data. Obviously, you must take into account which kind of this information you will have in your forecasting set.\n\n\r6. Set your project and code structure.\rHere we will show you our most common project structure. The directory structure of a Data Science CdU depends on the project nature, its development and production environments.\nProject:\n\rdata:\r\r1_raw:\r2_processed:\r\rmodels:\rnotebooks:\r\r1_eda:\r2_poc:\r3_modeling:\r4_evaluation:\r\rsrc:\r\r1_get_data:\r2_processing:\r3_modeling:\r4_evaluation:\r5_helpers:\r\r\rWe consider we can simplify this structure in the Datathon case, because we don’t need to automate the ETL or the assessment and validation process inside a productive workflow. In fact, the exploratory analysis and the main script are the principal points here. Our main script calls to the preprocess, train, test and evaluation modules.\nDatathon_Project:\n\rdata:\rexploratory:\rhelpers:\rlog:\rmain.R / main.py\routputs:\r\rmodels:\rpreds:\rvalidation:\r\r\rRegarding the main script structure, we usually build a custom cross validation with the objective of being flexible to train different models and stack its predictions. Our project template is as follows:\nLoading environment (packages, modules and functions)\rCrafting features\rSplit dataset and datasetOOSample\rSplit dataset into folds\rFor each fold in folds:\rTraining with the rest\rPredict in fold\rEvalutaion\r(In the last fold, stacking models training if you want it)\rOut of Sample prediction\rEvaluación (Base models and stacking comparaison)\rTest set prediction.\n\n\r7. Run your models.\rFirs of all, we advise you to focus in a model family and loss function that better fit a priori our data, response variable and evaluated metric. The first results let you to test your code and pipeline and will be the starting point for further improvements.\nYou can use Google to search about different algorithm families and loss functions according your data and objective.\rAlso, there such an amazing info in Kaggle kernels, Reddit feed and YouTube speaks.\n\n\r8. Cross Validation, OOS and backtesting.\rSpecial attention to the error metrics. One of the goals that we set in our data competitions is to know as precisely as possible the error interval (confidence or prediction) of our models before we submit predictions.\nTo know about our forecast uncertainity behind several scenarios, data subsets and synthetic data make us feel proud. Also, it could helps us to be sure about our error metrics improvements are not due to spurious behaviour or bugs.\n\n\r9. Model interpretability\rIn some data competitions, the best teams have to expose their projects in a final event. Here, in addition to talk about your data flow and research done, you probably want to talk about some insights and interpretable interactions between data and not only the model prediction.\nMany packages like SHAP or LIME integrate different modules focused on the model and predictions interpretability. These modules instead of looking for a “mean” variable importance in a global point of view, compose additionally a local feature contribution in each predicted observation.\n\n\r10. Write down your observed strengths and weaknesses. It will be the starting point for your next proyect. \rOnce you have finished the project and committed the predictions, writing down your strengths and weaknesses is a cool idea. These thoughts are such a fantastic starting point to the next competition or project.\n\nSo here is our contribution, hoping you will ace your competition! And the most important, have fun!!\n\rp {\rword-spacing: 3px;\rtext-indent: 20px;\rtext-align: justify;\r}\r.page-subtitle {\rtext-align: left !important;\rtext-indent: 0px !important;\r}\r.card-text {\rtext-align: left !important;\rtext-indent: 0px !important;\r}\r\r\rwindow.dojoRequire([\"mojo/signup-forms/Loader\"], function(L) { L.start({\"baseUrl\":\"mc.us4.list-manage.com\",\"uuid\":\"91551f7ed29389a0de4f47665\",\"lid\":\"d95c503a48\",\"uniqueMethods\":true}) })\r\r","date":1583884800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583884800,"objectID":"7d24c3b54d708476d634af4418447b3d","permalink":"/post/10-tips-datathon-kaggle-data-science/","publishdate":"2020-03-11T00:00:00Z","relpermalink":"/post/10-tips-datathon-kaggle-data-science/","section":"post","summary":"Do you want to achieve your goals?","tags":[],"title":"10 Tips to ace your Kaggle, Datathon or any data competition!","type":"post"},{"authors":["Carlos Vecina"],"categories":["R","Post"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rHere you can find the R code. It’s an ‘in progress’ script. I will develop basic features like:\n\rploting/rendering code refactoring;\n\rGGanimate with the algortihm steps moving forward;\n\rand, of course, several pathfinding algorithms and evolutionary ones.\n\r\rAll these features and much more in following posts! Stay tuned! \nBelow you can find the main code you can use to template. As I said a refactor is needed, but you can extract te core logic and functionalities of the interactive R Shiny framework:\nlibrary (shiny)\rlibrary (ggplot2)\rlibrary (tidyverse)\rsource(\u0026quot;helpers/ColourBorders.R\u0026quot;)\rsource(\u0026quot;helpers/PlotMapGrid.R\u0026quot;)\r\r\rui \u0026lt;-fluidPage(\rmainPanel(\rcolumn(12,offset = 5, \rtitlePanel(\u0026quot;Pathfinding Algorithm Visualization using R!\u0026quot;)),\rHTML(\u0026quot;\u0026amp;nbsp\u0026quot;),\rcolumn(12,offset = 5,HTML(\u0026quot;\u0026amp;nbsp\u0026quot;),\ractionButton(\u0026quot;go_search_actionButton\u0026quot;, \u0026quot;Go Search!\u0026quot;),\ractionButton(\u0026quot;clean_all_actionButton\u0026quot;, \u0026quot;Clean All\u0026quot;)),\rHTML(\u0026quot;\u0026amp;nbsp\u0026quot;),\rcolumn(12,offset=5, plotOutput(\u0026quot;map_grid_plotOutput\u0026quot;,\rclick=\u0026quot;map_grid_plotOutput_click\u0026quot;))\r))\r\r\rserver \u0026lt;-function(input, output){\r\r## Initial params\rmax_steps \u0026lt;-50\rmatrix_x_size \u0026lt;-20\rmatrix_y_size \u0026lt;-20\rgrid_map_reactive \u0026lt;-matrix(ncol = matrix_x_size,\rnrow = matrix_y_size,\rdata = 0) \r\r## Colours Dict (in progress)\r# 1- Wall\r# 2- Init\r# 3- Obj\r# 4- Step done\r# 5- Goal achieved\r\r# Initialize objts\rgrid_map_reactive[4,15] \u0026lt;-3 # obj\rgrid_map_reactive[17,3] \u0026lt;-2 # init\rinitial_step \u0026lt;-which(grid_map_reactive ==2,\rarr.ind = TRUE)\rgrid_map_reactive \u0026lt;-ColourBorders(grid_map_reactive, 1) # rounding walls\rreact_df \u0026lt;-reactiveValues(df = grid_map_reactive, # reactive init\rorig = grid_map_reactive,\rwalls = grid_map_reactive)\r\robserve({\rif(!is.null(input$map_grid_plotOutput_click)){\rnew_x_value \u0026lt;-trunc(input$map_grid_plotOutput_click$x)\rnew_y_value \u0026lt;-trunc(input$map_grid_plotOutput_click$y)\r\rif(between(new_x_value,2,matrix_x_size-1) \u0026amp;between(new_y_value,2,matrix_y_size-1)){\risolate(react_df$df[new_y_value,new_x_value] \u0026lt;-if_else(react_df$df[new_y_value,new_x_value]==0,\r1,0))\risolate(react_df$df[4,15] \u0026lt;-3)\risolate(react_df$df[17,3] \u0026lt;-2)\risolate(react_df$df[17,3] \u0026lt;-2)\risolate(react_df$walls \u0026lt;-react_df$df)\r\routput$map_grid_plotOutput \u0026lt;-renderPlot({\r\rPlotMapGrid(react_df$df,\rmatrix_x_size,\rmatrix_y_size)\r\r}, width=600, height=600,position=\u0026quot;center\u0026quot;)\r}}\r}) \r\r# Go search! Pseudo-random pathfinding algortihm\robserveEvent(input$go_search_actionButton,{\r\rif(nrow(which(react_df$df ==4, arr.ind = TRUE))\u0026gt;=1) react_df$df \u0026lt;-react_df$walls # click search without clean\rcurrent_step \u0026lt;-initial_step \robj \u0026lt;-which(react_df$df ==3, arr.ind = TRUE)\rprevious_steps_with_opt \u0026lt;-current_step\r\rfor(i in 1:max_steps){\rnext_step_col \u0026lt;-tribble(~row, ~col,\rcurrent_step[1]+1,current_step[2]+0,\rcurrent_step[1]+0,current_step[2]+1,\rcurrent_step[1]-1,current_step[2]+0,\rcurrent_step[1]+0,current_step[2]-1)\rnext_values \u0026lt;-NULL\r\rfor(r in 1:nrow(next_step_col)){\rnext_values \u0026lt;-c(next_values,\rreact_df$df[next_step_col[[r,1]],\rnext_step_col[[r,2]]])\r}\rif(3 %in%next_values){\r\rcurrent_step \u0026lt;-next_step_col[next_values==3,] %\u0026gt;%\ras.matrix()\r\rreact_df$df[current_step] \u0026lt;-5\r\rbreak()\r\r} else if(0 %in%next_values){\r\rif(sum(next_values==0)\u0026gt;1){\r\rprevious_steps_with_opt \u0026lt;-current_step\r\r}\r\rcurrent_step \u0026lt;-next_step_col[next_values==0,] %\u0026gt;%\rsample_n(1) %\u0026gt;%\ras.matrix()\r\rreact_df$df[current_step] \u0026lt;-4\r\r} else {\r\rcurrent_step \u0026lt;-previous_steps_with_opt\r\r}\r}\r})\r\r# Reset all\robserveEvent(input$clean_all_actionButton,{\r\rreact_df$df \u0026lt;-react_df$orig\rreact_df$walls \u0026lt;-react_df$orig\r\r})\r\r# First panel\routput$map_grid_plotOutput \u0026lt;-renderPlot({\r\rPlotMapGrid(react_df$df,\rmatrix_x_size,\rmatrix_y_size)\r\r}, width=550, height=600,position=\u0026quot;center\u0026quot;)\r\r}\r\r\rshinyApp(ui=ui, server = server)\rHere are the helpers to construct the grid:\nColourBorders \u0026lt;-function(df, col_value){\r\r## Rounding walls \r# Params: df - Map grid\r# col_value - Colour to fill the rounding blocks\r# Return: df with the filled roundings\r\rdf[1,] \u0026lt;-col_value\rdf[,1] \u0026lt;-col_value\rdf[nrow(df),] \u0026lt;-col_value\rdf[,ncol(df)] \u0026lt;-col_value\r\rreturn(df)\r\r}\r\rPlotMapGrid \u0026lt;-function(df, matrix_x_size, matrix_y_size){\r\r## Plot the interactive grid \r# Params: df - Map grid\r# matrix_x_size - X_axis limit\r# matrix_y_size - Y_axis limit\r# Return: plot with the pathfinding\r\r\rplot \u0026lt;-rbind(\rwhich(df==1, arr.ind = TRUE) %\u0026gt;%cbind(fill_col=\u0026quot;#623B17\u0026quot;),\rwhich(df ==2, arr.ind = TRUE) %\u0026gt;%cbind(fill_col=\u0026quot;#13293D\u0026quot;),\rwhich(df ==3, arr.ind = TRUE) %\u0026gt;%cbind(fill_col=\u0026quot;#ffff66\u0026quot;),\rwhich(df ==4, arr.ind = TRUE) %\u0026gt;%cbind(fill_col=\u0026quot;#99ccff\u0026quot;),\rwhich(df ==5, arr.ind = TRUE) %\u0026gt;%cbind(fill_col=\u0026quot;#1B998B\u0026quot;)\r\r) %\u0026gt;%\rdata.frame(stringsAsFactors = F) %\u0026gt;%\rtransmute(y = as.numeric(row), x = as.numeric(col), fill_col=fill_col) %\u0026gt;%\rggplot(aes(x+0.5,y+0.5)) +\rgeom_tile(width = 1, height = 1, fill = df$fill_col, col=\u0026quot;black\u0026quot;) +\rscale_y_reverse() +\rscale_x_continuous(breaks = seq(0, matrix_x_size, 1),\rlimits = c(0+0.5, matrix_x_size+1.5), \rminor_breaks = NULL) +\rscale_y_continuous(breaks = seq(0, matrix_y_size, 1),\rlimits = c(0+0.5, matrix_y_size+1.5),\rminor_breaks = NULL) +\rtheme_linedraw()+\rtheme(axis.title.x=element_blank(),\raxis.title.y=element_blank(),\raxis.text.x=element_blank(),\raxis.text.y=element_blank(),\raxis.ticks.x=element_blank(),\raxis.ticks.y=element_blank())\r\rreturn(plot)\r\r}\r\nYou can find the Pathfinding Visualization using R series on TypeThePipe\n\n\rbody {\rtext-align: justify}\rp {\rword-spacing: 3px;\r}\r\r\rwindow.dojoRequire([\"mojo/signup-forms/Loader\"], function(L) { L.start({\"baseUrl\":\"mc.us4.list-manage.com\",\"uuid\":\"91551f7ed29389a0de4f47665\",\"lid\":\"d95c503a48\",\"uniqueMethods\":true}) })\r","date":1579564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579564800,"objectID":"3cd4ca45b606003c1f23e99028a848a4","permalink":"/post/pathfinding-algorithms-visualizer-using-r/","publishdate":"2020-01-21T00:00:00Z","relpermalink":"/post/pathfinding-algorithms-visualizer-using-r/","section":"post","summary":"Setting up the interactive grid with Shiny and ggplot! Trail with some kind of random-walker algorithm.","tags":[],"title":"Pathfinding Algorithms Visualizer using R! (I) Setting up the interactive grid","type":"post"},{"authors":["Pablo Cánovas"],"categories":["R","Tidyverse","Tips"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rSome time ago I made one of the best discoveries I have ever made in the Tidyverse: a tool called tidylog.\rThis package is built on top of dplyr and tidyr and provides us with feedback on the results of the operations. Actually, this is a feature that already appeared in the Stata software.\nWhen performing one operation at a time, it is easy to track the changes made on a table.\rHowever things get increasingly obscure when chaining multiple functions or dealing with big data frames.\nWe all love piping operations. I often ‘play’ to perform the whole transformation without leaving the pipeflow. But the counterpart is missing the intermediate states: you can make some big mistakes and be unaware of them until it’s too late and maybe you have to undone some work or rethink your analysis.\nIn this context, some additional info is always welcome. I think this feature is specially convenient for beginners, but not only! I have myself wasted several hours debugging long pipelines and trying to understand where the problems came from.\nLet’s see a tiny bit of its behaviour with a simple example:\ndiv.sourceCode {\roverflow-x: hidden;\r}\r\rlibrary(nycflights13)\rlibrary(tidyverse)\rlibrary(tidylog)\r\rflights %\u0026gt;%\rselect(year:day, hour, origin, dest, tailnum, carrier) %\u0026gt;%\rmutate(month = if_else(nchar(month) ==1, paste0(\u0026quot;0\u0026quot;,month), as.character(month)),\rday = if_else(nchar(day) ==1, paste0(\u0026quot;0\u0026quot;,day), as.character(day))) %\u0026gt;%\runite(\u0026quot;date\u0026quot;, year:day, sep = \u0026quot;/\u0026quot;, remove = T) %\u0026gt;%\rmutate(date = lubridate::ymd(date)) %\u0026gt;%\rfilter(hour \u0026gt;=8) %\u0026gt;%\ranti_join(planes, by = \u0026quot;tailnum\u0026quot;) %\u0026gt;%\rcount(tailnum, sort = TRUE) \r\r# select: dropped 11 variables (dep_time, sched_dep_time, dep_delay, arr_time, sched_arr_time, …)\r# mutate: converted \u0026#39;month\u0026#39; from integer to character (0 new NA)\r# converted \u0026#39;day\u0026#39; from integer to character (0 new NA)\r# mutate: converted \u0026#39;date\u0026#39; from character to double (0 new NA)\r# filter: removed 50,726 rows (15%), 286,050 rows remaining\r# anti_join: added no columns\r# \u0026gt; rows only in x 45,008\r# \u0026gt; rows only in y ( 39)\r# \u0026gt; matched rows (241,042)\r# \u0026gt; =========\r# \u0026gt; rows total 45,008\r# count: now 716 rows and 2 columns, ungrouped\rPretty neat! It is specially useful with joins, as it provides plenty of details and they can be a source of duplicated or missing rows.\nI decided to write this little post now to celebrate that tidylog v1.0.0 has recently been released! Check the official repo out to see more examples or show some love to @elbersb on Twitter!\nAll in all, I think this package was a missing piece in the Tidyverse ecosystem: It is incredibly useful, whereas making advantage of it is as simple as writing library(tidylog). Integrating this package into our daily R work is a no-brainer!\n","date":1579564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579564800,"objectID":"0c2f957a6e20d268bf1b4b3da5a7ff2a","permalink":"/vizs-and-tips/tidylog-logging-pipelines/","publishdate":"2020-01-21T00:00:00Z","relpermalink":"/vizs-and-tips/tidylog-logging-pipelines/","section":"vizs-and-tips","summary":"Logging your pipelines","tags":[],"title":"Tidylog","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["R","Tips"],"content":"\r\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rSupose you are analysing survey data. You are asked to get the mean in a representative way, weighting your individuals depending on the number of members of their segment.\nlibrary(tidyverse)\r\rsurvey_data \u0026lt;-tribble(\r~id, ~region1, ~region2, ~gender, ~q1, ~q2,\r1,\u0026quot;sp\u0026quot;,\u0026quot;mad\u0026quot;,\u0026quot;m\u0026quot;, 2,5,\r2,\u0026quot;it\u0026quot;, \u0026quot;bol\u0026quot;, \u0026quot;m\u0026quot;, 5, 10,\r3,\u0026quot;sp\u0026quot;, \u0026quot;bar\u0026quot;, \u0026quot;f\u0026quot;, 2, 2,\r4,\u0026quot;sp\u0026quot;, \u0026quot;bar\u0026quot;, \u0026quot;f\u0026quot;, 7, 7,\r5,\u0026quot;it\u0026quot;, \u0026quot;bol\u0026quot;, \u0026quot;m\u0026quot;, 2, 7) \rsurvey_data %\u0026gt;%\rgroup_by(region1, region2, gender) %\u0026gt;%\rmutate(weight = 1/n()) %\u0026gt;%\rungroup() %\u0026gt;%\rsummarise_at(vars(contains(\u0026quot;q\u0026quot;)),\rfuns(weighted_mean = sum(. *weight)/sum(weight)))\r\r\rq1_weighted_mean\r\rq2_weighted_mean\r\r\r\r\r\r3.333333\r\r6\r\r\r\r\r","date":1579132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579132800,"objectID":"d9f06b3f3736c611c9372ca8190dde00","permalink":"/vizs-and-tips/summarise_at-weighted-mean-tidyverse-r/","publishdate":"2020-01-16T00:00:00Z","relpermalink":"/vizs-and-tips/summarise_at-weighted-mean-tidyverse-r/","section":"vizs-and-tips","summary":"Survey analysis using R","tags":[],"title":"Using summarise_at(). Weighted mean Tidyverse approach","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["Python","Tips"],"content":"\rA recurrent complaint beginner users have about Jupyter Notebooks is the lack of information about the self-created environment variables. If you have this question, probably first of all you should be sure about the Jupyter Notebooks main purpose (totally different from IDEs like Spyder or Pycharm).\nBut in case a notebook is all what you need, you have few ways to display this information. The first and easiest one is to use the magic method %whos\rTwo alternative ways are nbextension and Jupyter Lab variable inspector. You can find more information here\n","date":1578960000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578960000,"objectID":"6d23f87878690596211988dedd6c93c7","permalink":"/vizs-and-tips/magic-whos-method-jupyter-notebook/","publishdate":"2020-01-14T00:00:00Z","relpermalink":"/vizs-and-tips/magic-whos-method-jupyter-notebook/","section":"vizs-and-tips","summary":"Kind of magic","tags":[],"title":"List all the defined variables in Jupyter Notebooks","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["R","Vizs"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\r\nIn this TypeThePipe tip we are bringing you a skills plot template using R and ggplot2. Maybe its a good idea to evolve this plot and add an unique skill plot to your CV. And it’s only a few lines of R code!\nYou can see the code below :)\nlibrary(ggplot2)\r# library(plotly)\rlibrary(tibble)\rlibrary(dplyr)\r\rskills \u0026lt;-tribble(\r~Skill, ~Hours, ~Class,\r\u0026quot;AWS\u0026quot;, 500, \u0026quot;BigData\u0026quot;,\r\u0026quot;Python\u0026quot;, 8000, \u0026quot;Language\u0026quot;,\r\u0026quot;Spark\u0026quot;, 4000, \u0026quot;BigData\u0026quot;,\r\u0026quot;R\u0026quot;, 9000, \u0026quot;Language\u0026quot;,\r\u0026quot;Git\u0026quot;, 2000, \u0026quot;Tools\u0026quot;,\r\u0026quot;Jira\u0026quot;, 2000, \u0026quot;Tools\u0026quot;,\r\u0026quot;Forecasting\u0026quot;, 5000, \u0026quot;Objetive\u0026quot;,\r\u0026quot;Segmentation\u0026quot;, 2000, \u0026quot;Objetive\u0026quot;,\r\u0026quot;Computer Vision\u0026quot;, 600, \u0026quot;Objetive\u0026quot;,\r\u0026quot;SQL\u0026quot;, 4500, \u0026quot;Language\u0026quot;,\r\u0026quot;IBM Data Stage \u0026amp; SPSS\u0026quot;, 1200, \u0026quot;Tools\u0026quot;,\r\u0026quot;Shiny R\u0026quot;, 1500, \u0026quot;Visualization\u0026quot;,\r\u0026quot;Tableau\u0026quot;, 1000, \u0026quot;Visualization\u0026quot;,\r\u0026quot;Spotfire\u0026quot;, 500, \u0026quot;Visualization\u0026quot;\r) \r# plotly(\rggplot(data=skills,aes(x=reorder(Skill,-desc(Hours)), y= Hours, fill=Class, label=paste0(Hours,\u0026quot; h\u0026quot;))) +\rgeom_bar(stat = \u0026quot;identity\u0026quot;, colour=\u0026quot;black\u0026quot;) +\rcoord_flip() +\rlabs(x=\u0026quot; \u0026quot;, y=\u0026quot;Hours\u0026quot;, fill=\u0026quot; \u0026quot;) +\rtheme_minimal() +\rscale_fill_brewer(palette = \u0026quot;YlOrBr\u0026quot;,direction = -1) +\rgeom_label(show_guide = F, aes(y=400)) # Use show_guide despite the warning\r\nTune up your R visualizations on TypeThePipe\n\n","date":1578355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578355200,"objectID":"2ba02a1580e0ba11ef62a4b5905e8cf7","permalink":"/vizs-and-tips/skills-plot-curriculum-r-ggplot/","publishdate":"2020-01-07T00:00:00Z","relpermalink":"/vizs-and-tips/skills-plot-curriculum-r-ggplot/","section":"vizs-and-tips","summary":"Skills snapshot in 6 Ggplot2 lines","tags":[],"title":"Skills chart using Gplot2 with R","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["R","Tips"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rLast week several gganimate visualizations came to my feed. Some R users were wondering about reordering gganimate and ggplot2 bars as long as them are evolving (over animation time).\nThen, we came up with this R viz where several bars are not only evolving and reordering over time but leaving and joining the chart. We want the top 4 countries over time, then the remaining countries in each timestamp should leave.\nYou can achieve this effect and this kind of gganimate visualization following this commented code:\nlibrary(ggplot2)\rlibrary(gganimate)\rlibrary(tidyverse)\rdf_evolution_data \u0026lt;-data.frame(Name=rep(c(\u0026quot;Madrid\u0026quot;,\u0026quot;Barcelona\u0026quot;,\r\u0026quot;Valencia\u0026quot;,\u0026quot;Alicante\u0026quot;,\r\u0026quot;Sevilla\u0026quot;),5),\rYear = factor(sort(rep(2001:2005, 5))),\rValue = runif(25,100,1000))\r\rdf_evolution_data_filtered \u0026lt;-df_evolution_data %\u0026gt;%\rgroup_by(Year) %\u0026gt;%\rmutate(Rank = rank(Value)) %\u0026gt;%# Rank 1 the lowest 5 the higest\rfilter(Rank \u0026gt;=2) # Top 4 countries\rggplot(df_evolution_data_filtered) +\rgeom_col(aes(x=Rank, y=Value,\rgroup=Name, fill=Name),\rwidth=0.4) +\rgeom_text(aes(x=Rank, y=0,\rlabel=Name, group=Name),\rhjust=1.25) +\rtheme_minimal() +ylab(\u0026#39;Value\u0026#39;) +\rtheme(axis.title.y = element_blank(),\raxis.text.y = element_blank(),\raxis.ticks.y = element_blank(),\rplot.margin = unit(c(5,5,5,5),\r\u0026#39;lines\u0026#39;)) +\rscale_fill_brewer(palette=\u0026quot;Dark2\u0026quot;) +\rcoord_flip(clip=\u0026#39;off\u0026#39;) +\rggtitle(\u0026#39;{closest_state}\u0026#39;) +# title with the timestamp period\rtransition_states(Year,\rtransition_length = 1,\rstate_length = 1) +\rexit_fly(x_loc = 0, y_loc = 0) +# chart exit animation params\renter_fly(x_loc = 0, y_loc = 0) # chart enter animation params\r\nTune up your R visualizations on TypeThePipe\n\n","date":1576454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576454400,"objectID":"90ec1d6400b13e773818b1e133f3e258","permalink":"/vizs-and-tips/reorder-bars-r-ggplot-gganimate/","publishdate":"2019-12-16T00:00:00Z","relpermalink":"/vizs-and-tips/reorder-bars-r-ggplot-gganimate/","section":"vizs-and-tips","summary":"Reorder your bar groups over time in gganimate plot.","tags":[],"title":"Reordering bars in GGanimate visualization","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["R","Tips"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\r\nYesterday I was asked to easily plot confidence intervals at ggplot2 chart. Then I came up with this shadowing ggplot2 feature called geom_ribbon().\nIt’s not a trivial issue as long as you need to gather your data in order to achieve a tidy format. When you already have this data frame, all you need is geom_ribbon().\nBy using the following commented code you are able to show not only your point estimated forecast but also its confidence or prediction intervals.\nlibrary(tidyverse)\rhuron \u0026lt;-data.frame(year = 1875:1972, \rvalue = LakeHuron,\rstd = runif(length(LakeHuron),0,1))\r\rhuron %\u0026gt;%\rggplot(aes(year, value)) +\rgeom_ribbon(aes(ymin = value -std,\rymax = value +std), # shadowing cnf intervals\rfill = \u0026quot;steelblue2\u0026quot;) +\rgeom_line(color = \u0026quot;firebrick\u0026quot;,\rsize = 1) # point estimate\rFor a multi-line plot, you should include the colour and group aesthetic as follows:\nlibrary(tidyverse)\rhuron \u0026lt;-data.frame(year = rep(1875:1972,2), \rgroup = c(rep(\u0026quot;a\u0026quot;,98),rep(\u0026quot;b\u0026quot;,98)),\rvalue = c(LakeHuron, LakeHuron +5),\rstd = runif(length(LakeHuron)*2,0,1))\r\rhuron %\u0026gt;%\rggplot(aes(year, value, fill = group)) +\rgeom_ribbon(aes(ymin = value -std,\rymax = value +std,\rgroup=group),\rfill = \u0026quot;steelblue2\u0026quot;) +\rgeom_line(color = \u0026quot;firebrick\u0026quot;, size = 1)\r\nYou can see related R visualizations and tips on TypeThePipe\n\n","date":1574035200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574035200,"objectID":"8988f7c80ec7e20f310bc9c4566f262e","permalink":"/vizs-and-tips/ggplot-geom_ribbon-shadow-confidence-interval/","publishdate":"2019-11-18T00:00:00Z","relpermalink":"/vizs-and-tips/ggplot-geom_ribbon-shadow-confidence-interval/","section":"vizs-and-tips","summary":"Plot your confidence intervals easily","tags":[],"title":"Shadowing your ggplot lines. Forecasting confidence interval use case.","type":"vizs-and-tips"},{"authors":["Pablo Cánovas"],"categories":["R","Tidyverse","Tips"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rHow could we apply certain functions conditionally without leaving the pipeflow?\rThis way:\ndf %\u0026gt;%{ if(apply_filter ==TRUE) filter(., condition) else . } %\u0026gt;%...\r","date":1572652800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572652800,"objectID":"0e8f0cc2150f52d9da0b02ac03dae236","permalink":"/vizs-and-tips/conditional-pipes-r-tidyverse/","publishdate":"2019-11-02T00:00:00Z","relpermalink":"/vizs-and-tips/conditional-pipes-r-tidyverse/","section":"vizs-and-tips","summary":"Don't leave the pipeflow","tags":[],"title":"Conditional Pipes","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["Tips","Datathon"],"content":"\r\nKaggle - https://www.kaggle.com\nDriven Data - https://www.drivendata.org/competitions/\nCodaLab - https://competitions.codalab.org/\nAlibaba Tianchi - https://tianchi.aliyun.com/competition/gameList/activeList\nAnalyticsvidhya - https://datahack.analyticsvidhya.com/contest/all\nhackerearth - https://www.hackerearth.com/challenges/competitive/\nAgorize - https://www.agorize.com/en/challenges\n\nThere are also international events you might know:\n\nIDA (international Data Analysis Olympiad) (Registro en Enero) - https://idao.world/\nIronViz - https://www.tableau.com/iron-viz\nTopCoder - https://tco19.topcoder.com/competition-overview/algorithm\nData Mining Cup - https://www.data-mining-cup.com/\n\nNowadays, many national and local competitions are launched. We recommend you to search about them!\n\r\rwindow.dojoRequire([\"mojo/signup-forms/Loader\"], function(L) { L.start({\"baseUrl\":\"mc.us4.list-manage.com\",\"uuid\":\"91551f7ed29389a0de4f47665\",\"lid\":\"d95c503a48\",\"uniqueMethods\":true}) })\r","date":1570406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570406400,"objectID":"b5d2a0df261c0114223712a65f18d657","permalink":"/vizs-and-tips/data-competition-platforms-beyond-kaggle/","publishdate":"2019-10-07T00:00:00Z","relpermalink":"/vizs-and-tips/data-competition-platforms-beyond-kaggle/","section":"vizs-and-tips","summary":"Kaggle is not all there!","tags":[],"title":"Data competition platforms","type":"vizs-and-tips"},{"authors":["Pablo Cánovas"],"categories":["R","Tidyverse","Tips"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rSome times you need to filter a data frame applying the same condition over multiple columns. Obviously you could explicitly write the condition over every column, but that’s not very handy.\nFor those situations, it is much better to use filter_at in combination with all_vars.\nImagine we have the famous iris dataset with some attributes missing and want to get rid of those observations with any missing value.\ndiv.sourceCode {\roverflow-x: hidden;\r}\r\r# # A tibble: 10 x 6\r# rowid Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r# \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \r# 1 1 5.1 NA 1.4 0.2 setosa \r# 2 2 NA 3 1.4 NA setosa \r# 3 3 4.7 3.2 1.3 0.2 setosa \r# 4 4 NA 3.1 1.5 0.2 setosa \r# 5 5 5 3.6 1.4 0.2 setosa \r# 6 6 5.4 3.9 1.7 0.4 setosa \r# 7 7 4.6 3.4 1.4 0.3 setosa \r# 8 8 NA 3.4 1.5 0.2 setosa \r# 9 9 4.4 2.9 1.4 0.2 setosa \r# 10 10 NA NA NA NA setosa \rWe could write the condition on every column, but that would cumbersome:\niris %\u0026gt;%\rfilter(!is.na(Sepal.Length) \u0026amp;\r!is.na(Sepal.Width) \u0026amp;\r!is.na(Petal.Length) \u0026amp;\r!is.na(Petal.Width))\rInstead, we just have to select the columns we will filter on and apply the condition:\nfeatures \u0026lt;-iris %\u0026gt;%names() %\u0026gt;%keep(~str_detect(.,\u0026quot;[.]\u0026quot;))\riris %\u0026gt;%filter_at(vars(features), all_vars(!is.na(.)))\r# # A tibble: 5 x 6\r# rowid Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r# \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \r# 1 3 4.7 3.2 1.3 0.2 setosa \r# 2 5 5 3.6 1.4 0.2 setosa \r# 3 6 5.4 3.9 1.7 0.4 setosa \r# 4 7 4.6 3.4 1.4 0.3 setosa \r# 5 9 4.4 2.9 1.4 0.2 setosa \rHere we have used the function all_vars in the predicate to explicit that\revery feature must satisfy the condition.\rTo be honest, for that purpose it would have been easier to simply use iris %\u0026gt;% na.omit().\nBut what if we wanted the opposite? Keeping only the rows with all the selected features missing is as easy as changing the predicate part:\niris %\u0026gt;%filter_at(vars(features), all_vars(is.na(.)))\r\r# # A tibble: 1 x 6\r# rowid Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r# \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \r# 1 10 NA NA NA NA setosa \rAnother option is to apply the condition on any feature. That’s where any_vars comes handy. Here we keep only the observations with at least one missing feature:\niris %\u0026gt;%filter_at(vars(features), any_vars(is.na(.)))\r\r# # A tibble: 5 x 6\r# rowid Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r# \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \r# 1 1 5.1 NA 1.4 0.2 setosa \r# 2 2 NA 3 1.4 NA setosa \r# 3 4 NA 3.1 1.5 0.2 setosa \r# 4 8 NA 3.4 1.5 0.2 setosa \r# 5 10 NA NA NA NA setosa \rAlso, there are some other fancy ways to manipulate data frames with the filter family. One trick is using contains() or starts_with() to select the variables:\niris %\u0026gt;%filter_at(vars(contains(\u0026quot;Length\u0026quot;)), all_vars(. \u0026gt;=1.4))\r\r# # A tibble: 5 x 6\r# rowid Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r# \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \r# 1 1 5.1 NA 1.4 0.2 setosa \r# 2 5 5 3.6 1.4 0.2 setosa \r# 3 6 5.4 3.9 1.7 0.4 setosa \r# 4 7 4.6 3.4 1.4 0.3 setosa \r# 5 9 4.4 2.9 1.4 0.2 setosa \rAnother example is applying the condition on columns that satisfy certain condition with filter_if (notice the rowid fetaure here):\niris %\u0026gt;%filter_if(is.numeric, any_vars(. \u0026gt;5))\r\r# # A tibble: 6 x 6\r# rowid Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r# \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \r# 1 1 5.1 NA 1.4 0.2 setosa \r# 2 6 5.4 3.9 1.7 0.4 setosa \r# 3 7 4.6 3.4 1.4 0.3 setosa \r# 4 8 NA 3.4 1.5 0.2 setosa \r# 5 9 4.4 2.9 1.4 0.2 setosa \r# 6 10 NA NA NA NA setosa \r","date":1570233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570233600,"objectID":"099de6d08d215cc98583d6f1167c23db","permalink":"/vizs-and-tips/filtering-data-frame-by-condition-on-multiple-columns/","publishdate":"2019-10-05T00:00:00Z","relpermalink":"/vizs-and-tips/filtering-data-frame-by-condition-on-multiple-columns/","section":"vizs-and-tips","summary":"The magic of filter_at()","tags":[],"title":"Filtering a data frame by condition on multiple columns","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["R","Tips"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rAre you starting your data exploration? Do you want to have an easy overview of your variable NA percentage?\nWe create a function to benchmark different ways of achieving it:\nlibrary(microbenchmark)\rlibrary(tidyverse)\r\rbenchmark_count_na_by_column \u0026lt;-function(dataset){\rmicrobenchmark(\r# Summary table output\rdataset %\u0026gt;%summary(),\r# Numeric output\rcolSums(is.na(dataset)),\rsapply(dataset, function(x) sum(is.na(x))),\r# List output\rdataset %\u0026gt;%map(~sum(is.na(.))),\rlapply( dataset, function(x) sum(is.na(x))),\r# Df output\rdataset %\u0026gt;%\rselect(everything()) %\u0026gt;%\rsummarise_all(funs(sum(is.na(.)))),\rdataset %\u0026gt;%\rsummarise_each(funs(sum(is.na(.)))),\r# Tibble output\rdataset %\u0026gt;%map_df(~sum(is.na(.)))\r)\r}\rSee the performance dealing with small datasets:\nprint(airquality %\u0026gt;%nrow()) # 153 rows\rbenchmark_count_na_by_column(airquality)\r## Unit: microseconds\r##funct min lq mean median uq max neval class\r##summary() 1480.5 1582.60 1979.676 1897.30 2100.45 6403.2 100 table\r##colSums() 24.4 38.45 47.854 44.70 53.90 152.4 100 integer\r##sapply() 23.2 35.05 67.891 39.65 50.30 2494.8 100 integer\r##map() 140.2 182.60 214.092 200.75 238.50 549.6 100 list\r##lapply() 11.2 15.65 27.093 18.85 22.45 750.1 100 list\r##summarise_all() 1996.9 2147.80 2650.223 2382.90 2798.55 8133.7 100 data.frame\r##summarise_each() 2277.9 2497.05 2951.477 2898.40 3080.65 7977.2 100 data.frame\r##map_df() 190.0 249.00 331.368 275.40 326.05 383 100 tbl_df\rLet’s see how well them scale with 100000 rows dataset:\nbig_dataset %\u0026gt;%nrow() # 100 000 rows\rbenchmark_count_na_by_column(big_dataset)\r## Unit: milliseconds\r##funct min lq mean median uq max neval class\r##summary() 113.7535 129.35070 138.716624 133.14050 143.45920 252.0149 100 table\r##colSums() 4.4280 5.31080 12.502741 5.65005 18.77570 124.8206 100 integer\r##sapply() 2.2452 3.03095 6.788395 3.15310 15.04010 18.6061 100 integer\r##map() 2.5950 3.28390 5.760602 3.38020 3.69445 19.4527 100 list\r##lapply() 2.2018 2.95700 6.219106 3.03605 3.62860 19.5514 100 list\r##summarise_all() 5.0982 5.85135 10.093431 6.05940 6.87070 127.5107 100 data.frame\r##summarise_each() 5.7251 6.16980 10.191426 6.33065 6.72210 125.2943 100 data.frame\r##map_df() 2.6913 3.42045 7.694863 3.56720 3.89715 122.2030 100 tbl_df\r\rwindow.dojoRequire([\"mojo/signup-forms/Loader\"], function(L) { L.start({\"baseUrl\":\"mc.us4.list-manage.com\",\"uuid\":\"91551f7ed29389a0de4f47665\",\"lid\":\"d95c503a48\",\"uniqueMethods\":true}) })\r","date":1569974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569974400,"objectID":"704af2123e373a156efe0124b7f41ad8","permalink":"/vizs-and-tips/counting-nas-by-column-r-benchmark/","publishdate":"2019-10-02T00:00:00Z","relpermalink":"/vizs-and-tips/counting-nas-by-column-r-benchmark/","section":"vizs-and-tips","summary":"Benchmarking several functions","tags":[],"title":"Counting NAs by column in R","type":"vizs-and-tips"},{"authors":["Pablo Cánovas"],"categories":["Metrics","Post"],"content":"\r\r\rIntroduction\rScale dependent error metrics\rPercentage-error metrics\rScale-free error metrics\rConclusions\rReferences\r\r\rIntroduction\rThe idea of this post comes from the different error metrics I have dealt with working with time series data and forecasting models.\nAmong other things, we make energy production forecasts of renewable power plants of different capacities and technologies.\rOur aim is to develop forecasting models that reduce the penalties caused by the deviations.\nMost of the models I work with are regression models, and therefore in this article I am focusing only on regression error metrics.\nUnfortunately, there is no absolute “right” accuracy metric.\rChoosing the right metric is a problem-specific matter, and it involves answering questions like:\n\rWhich decision will you base on the forecast?\rWhat are the consequences of a wrong forecast?\rWho is going to check and monitorize the errors?\rDo we care about the percentage error or about the magnitude of the deviation?\rDoes it makes any difference to over-forecast or under-forecast the variable of interest?\r\rAnswering the above questions lead us to determine we need to find a metric that:\n\rIs scale independent, so the errors are comparable between power plants.\rIs symmetric, as we do not want to weight the deviations differently according to their sign.\rExpress the error in absolute terms, so the error reflects the real-life imbalance costs.\rThe error calculated over different periods should be equivalent to the aggregated calculation over those periods individually.\r\rEach metric behaves in a certain way and therefore reflects in a unique manner the features of the models.\rDepending on their properties, we can classify the metrics in several categories.\rLet’s take a look at them:\n\rScale dependent error metrics\rMaybe the most popular and simple error metric is MAE:\nMAE:\rThe Mean Absolute Error is defined as:\n\\[ MAE = \\frac{1}{N}\\sum_{t=1}^{N} |A_t - F_t| \\]\nWhile the MAE is easily interpretable (each residual contributes proportionally to the total amount of error), one could argue that using the sum of the residuals is not the best choice, as we could want to highlight especially whether the model incur in some large errors.\n\rMSE \u0026amp; RMSE\rFor those cases, maybe MSE (Mean Squared Error) or RMSE (Root Mean Squared Error) are a better choice. Here the error grows quadratically and therefore extreme values penalize the metric to a greater extent.\n\\[ MSE = \\frac{1}{N} \\sum_{t=1}^{N} |A_t - F_t|^2 \\]\n\\[ RMSE = \\sqrt{MSE} \\]\nThe main problem with scale dependent metrics is that they are not suitable to compare errors from different sources.\nIn our case, the capacity of the power plants would determine the magnitude of the errors and therefore comparing them between facilities would not make much sense.\rThis is something we should try to avoid when choosing the metric.\n\r\rPercentage-error metrics\rNext group express the error in percentage terms.\nMAPE\rThe most widespread one is the Mean Absolute Percentage Error:\n\\[ MAPE_\\% = \\frac{1}{N}\\sum_{t=1}^{N} \\frac{|A_t - F_t|}{A_t} \\times 100\\]\nAs we said above, depending on our goal, MAPE could be suitable or not. From my point of view percentage error metrics have some major drawbacks.\rThey may give different values for two observations with the same absolute error, depending on whether they share the same actual value or not:\n\r\rCase\r\rActual\r\rForecast\r\rAbsoluteDifference\r\rMAPE\r\r\r\r\r\r1\r\r100\r\r150\r\r50\r\r50\r\r\r\r2\r\r150\r\r100\r\r50\r\r33\r\r\r\r\rBesides, MAPE diverges when actual values tend to zero.\rIn our case it is impractical, as this could lead to extreme cases such as:\n\r\rCase\r\rActual\r\rForecast\r\rAbsoluteDifference\r\rMAPE\r\r\r\r\r\r3\r\r1\r\r11\r\r10\r\r1000\r\r\r\r\rThat is an undesired behaviour for an error metric since we don’t want to assign huge errors to deviations that involve insignificant operating costs. That suggests a first strong conclusion:\n\rWe need to find error metrics that are aligned with our business goals.\n\rBesides, in the example above we can see that MAPE isn’t symmetric as it weights differently two residuals whether the forecast is above or below the actual value.\rThat idea of symmetry lead us to sMAPE.\n\rsMAPE\rTrying to solve that assymetry, an alternative to MAPE was proposed. It is called sMAPE, which stands for Symmetric Mean Absolute Percentage Error:\n\\[ sMAPE_\\% = \\frac{2}{N} \\sum_{t=1}^{N}\\frac{|A_t - F_t|}{|A_t| + |F_t|} \\times 100\\]\nHowever, against all odds Symmetric MAPE is not symmetric: as MAPE, it may present different values for the same absolute deviation:\n\r\rCase\r\rActual\r\rForecast\r\rAbsoluteDifference\r\rsMAPE\r\r\r\r\r\r1\r\r100\r\r150\r\r50\r\r20\r\r\r\r2\r\r100\r\r50\r\r50\r\r33\r\r\r\r\rFor our use case it is very inconvenient that the same absolute deviation may be quantified with two different error values.\nThis is a key question: We don’t want to minimize the percentage error but to minimize the economic losses due to forecast deviations, and they are exclusively connected to the sum of the absolute errors.\rTherefore, we should evaluate the accuracy based on that criteria.\nAs a final point, simply mention that some others have proposed the Log Ratio \\(ln(F_t/A_t)\\) as a better alternative to MAPE.\rYou can read a brief description in the previously mentioned sMAPE article or\ran extended discussion in the original paper by Chris Tofallis\n\r\rScale-free error metrics\rThese are error metrics that have been conveniently normalized to make them dimensionless.\nThe main advantages of these metrics are:\n\rSame absolute deviations lead to the same error.\rThey are symmetric.\rThey are comparable between power plants.\rThey are connected to our economic goals.\r\rNMAE\rFirst of all we have NMAE that stands for Normalized Mean Absolute Error.\rThis metric is specific to the energy forecasting business as it is normalized by the capacity C of the power plant, but one could generalize it to any other area provided that there exist an upper bound for the forecasts.\nNMAE is expressed as a percentage. It is our preferred metric as it is truly connected with the business goals, it’s easily interpretable and comparable between plants.\n\\[ NMAE_\\% = \\frac{1}{N}\\sum_{t=1}^{N} \\frac{|A_t - F_t|}{C} \\times 100\\]\nBesides, it shows the desirable property:\n\\[ NMAE_{p1 ∪ p2} = \\frac{1}{2}(NMAE_{p1} + NMAE_{p2})\\]\rGiven both periods have the same length. If not (e.g: consecutive months), you would only have to adjust by their relative length.\n\rThe real-life cost of a forecast error is proportional to the absolute value of the residuals.\n\rThe only case when this metric does not apply is whenever the capacity notion has no sense: If the range of the possible values is not bounded, what normalizing constant should I choose?\nThis would be the case when forecasting temperatures or electricity market prices.\rUsing MAE could be appropiate in these cases, as the units are in the same scale than the magnitude (ºC or €/MWh) and so the errors are easily interpretable, although they would not be truly comparable across different markets or locations.\n\rMAD/Sum Ratio\rIn energy-related businesses, I have spotted another error metric usually (and, as far as I know, wrongly) called WMAE.\rNow, WMAE should stand for “Weighted Mean Average Error”. However, the definition I stumbled upon several times was:\r\\[ \\frac{1}{N}\\frac{ \\sum_{t=1}^{N}{|A_t - F_t|}}{\\sum_{t=1}^{N}{A_t}} \\times 100\\]\nwhich is basically the MAE normalized by acummulated energy production.\nIt resembles MAD/Mean Ratio:\n\\[ MAD/Mean Ratio_\\% = \\frac{1}{N}\\frac{ \\sum_{t=1}^{N}{|A_t - F_t|}}{\\frac{1}{N}\\sum_{t=1}^{N}{A_t}} \\times 100\\]\nFrom my point of view, and as an analogy of the MAD/Mean Ratio, the first expression should be called MAD/Sum Ratio.\rTheir properties are similar:\n\rTheir range is [0, \\(\\infty\\)) for non-negative values, which may be difficult to interpret.\rThey both show the same asymmetry as MAPE: Different error values come from the same absolute difference between forecasts and actuals.\rSmall absolute deviations may be associated to big MAD/Mean or MAD/Sum Ratios, given the actual values are small.\r\rFor all those reasons, we insist on the idea that they are kind of disconnected from our loss function.\n\rMASE\rThere are other scale-free metrics. One of them is MASE (Mean Absolute Scaled Error), proposed by Rob J. Hyndman:\n\\[ MASE =\\frac{\\frac{1}{J} \\sum_j |A_j - F_j|} {\\frac{1}{T-1}\\sum_{t=2}^T |A_{t}-A_{t-1}|} \\]\nwhere the numerator is the error in the forecast period and the denominator is the MAE of the one-step “naive forecast method” on the training set, that is \\(F_t = A_{t-1}\\).\rBecause of that, MASE is a metric specifically designed for time series.\nAgain, whether it is suitable for your needs or not depends entirely on the problem.\rWhile it has certain interesting properties such as scale-independence, convergence when \\(A_t\\to0\\) and symmetry,\rin our case this metric isn’t optimal for several reasons:\n\rThe training series must be completed, i.e., with no gaps. In our case, sometimes we have some measurements missing.\rMASE is equal to 1 when the forecast performance is similar to the naive forecast in the training set. That implies a dependence with the historic period that isn’t always very convenient: if in any given moment we happen to recieve some missing historical production measurements, the accuracy of our model suddenly would change, which may be kind of unintuitive and difficult to track through time.\rMASE is unbounded on the upper side.\rIt doesn’t seem to be a friendly metric to use with non-technical people, such as clients or stakeholders: How large are the expected losses for a 1.2 MASE?\r\r\rEMAE\rI found yet another scale-free error metric in a recent paper from the Energy Department of the Politecnico di Milano.\nThey called it EMAE (Envelope-weighted Mean Absolute Error):\n\\[ EMAE_\\% = \\frac{1}{N}\\frac{ \\sum_{t=1}^{N}{|A_t - F_t|}}{\\sum_{t=1}^{N}{\\max(A_t, F_t)}} \\times 100\\]\nThis metric is quite similar to the MAD/Sum Ratio above but divides by the sum of the maximum between the forecast and the measured power for each observation. It is also expressed as a percentage. This function shows some nice properties:\n\rIt is scale-independent.\rIt is symmetric.\rIt maps absolute deviation to one unique value.\rIt is easily interpretable as its range is [0,100].\rIt doesn’t diverge in any point.\rIt’s a nice alternative to NMAE since a capacity value is not required.\r\rThis formula allows for a cool graphic interpretation of the error: the numerator matchs with the yellow area whereas the denominator corresponds to the sum of the blue and yellow areas:\n\r\rConclusions\r\rThere is no “best metric” to measure model performance. There are several metrics that highlight different characteristics.\n\rOne key aspect is to find error metrics that are connected with our objectives.\n\rSince in most cases the real-life cost of a forecast error is proportional to the absolute value of the residuals, the choice of the metric should be consistent with it.\n\rFor our case, NMAE presents the ideal characteristics of interpretability, stability and relation with our loss function that make it the optimal choice.\n\rEMAE is proposed as a nice alternative for the cases NMAE cannot be applied.\n\r\r\rReferences\r\rAnother look at forecast-accuracy metrics for intermittent demand\rForecasting: Principles and Practice\rErrors on percentage errors\rA new accuracy measure based on bounded relative error for time series forecasting\rComparison of Training Approaches for Photovoltaic Forecasts by Means of Machine Learning\rOn the assymetry of the symmetric MAPE\rA better measure of relative prediction accuracty for model selection and model estimation\rA Guide to Forecast Error Measurement Statistics and How to Use Them\r\r\r","date":1569024000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569024000,"objectID":"b7da47011eb6a6daa2c44d319d8bd90b","permalink":"/post/energy-forecasting-error-metrics/","publishdate":"2019-09-21T00:00:00Z","relpermalink":"/post/energy-forecasting-error-metrics/","section":"post","summary":"A discussion about the main error metrics and their properties","tags":[],"title":"Energy forecasting error metrics","type":"post"},{"authors":["Pablo Cánovas"],"categories":["Metrics","Post"],"content":"\r\rIntroduction\rI recently came across this interesting discussion about percentage errors and I would like to talk about MAPE and its characteristics.\nMAPE is the acronym of Mean Absolute Percentage Error and is defined as\n\\[MAPE = \\frac{100}{N} \\sum_{t=1}^{N} \\frac{|A_t - F_t|}{|A_t|}\\]\nbeing \\(A_t\\) the Actuals and \\(F_t\\) the Forecasts\nThere is some confusion and disagreement about the behaviour of MAPE.\nMAPE express the deviation in relative terms and provides a simple interpretation of the error.\rIt is easy to calculate and communicate and probably that is why it has been widely used in forecasting business.\nHowever, it suffers from some known issues.\n\rThe problems\r\rMAPE ranges from 0 to \\(\\infty\\): it diverges with \\(A_t \\to 0\\) which leads to problems when dealing with data with zero values such as intermittent demand data or energy forecasting.\rThis problem is even worse when working with data with arbitrary zero values, e.g, forecasting temperatures near 0\\(^o\\) in Celsius or Fahrenheit scale. To be fair, this would be an issue using any percentage error metric.\n\rMAPE is said to be asymmetric in the sense it puts heavier penalty on negative errors. When \\(F_t \u0026lt; A_t\\) the maximum possible error value is 100%,\rhowever there is no limit when forecasting on the high side. Besides, given the same absolute difference \\(|A_t - F_t|\\), MAPE is greater when \\(F_t \u0026gt; A_t\\):\n\r\r\r\rCase\r\rActual\r\rForecast\r\rAbsoluteDifference\r\rMAPE\r\r\r\r\r\r1\r\r100\r\r150\r\r50\r\r50\r\r\r\r2\r\r150\r\r100\r\r50\r\r33\r\r\r\r\rHowever, there is some controversy over this last point.\rSome people argue this is a false dichotomy because it doesn’t make sense to compare two situations where you are exchanging forecast and actual values, and defend MAPE actually is symmetric because you can’t get a lower MAPE just by lowering your forecasts:\n\r\rCase\r\rActual\r\rForecast\r\rAbsoluteDifference\r\rMAPE\r\r\r\r\r\r1\r\r100\r\r150\r\r50\r\r50\r\r\r\r2\r\r100\r\r50\r\r50\r\r50\r\r\r\r\r\rThe…solution?\rTrying to solve the alledgelly asymmetry, some alternative versions have been proposed. The more general one is:\n\\[ sMAPE = \\frac{200}{N} \\sum_{t=1}^{N}\\frac{|A_t - F_t|}{|A_t| + |F_t|} \\]\nFirst thing to notice is that the range of this symmetric MAPE is \\(\\big[0,200 \\big]\\) which is somewhat antiintuitive. I can’t see myself explaining model deviations in such metric to anybody with a non-technical background.\rThis could be solved simply dividing by 2, although that would be an aesthetic change only.\rOn the bright side, this version of sMAPE doesn’t diverge, which brings some sanity and stability to the metric.\nBut the aspect that really fascinates me is that the so-called symmetric MAPE is not symmetric.\rIn fact, sMAPE symmetrizes the asymmetric case above, and the other way around:\n\rExchanging actual and forecast values does produce the same sMAPE value.\r\r\r\rCase\r\rActual\r\rForecast\r\rAbsoluteDifference\r\rMAPE\r\rsMAPE\r\r\r\r\r\r1\r\r100\r\r150\r\r50\r\r50\r\r20\r\r\r\r2\r\r150\r\r100\r\r50\r\r33\r\r20\r\r\r\r\r\rModifying the forecast while holding fixed actual values and absolute deviation do not produce the same sMAPE value. This maybe the most important case.\r\r\r\rCase\r\rActual\r\rForecast\r\rAbsoluteDifference\r\rMAPE\r\rsMAPE\r\r\r\r\r\r1\r\r100\r\r150\r\r50\r\r50\r\r20\r\r\r\r2\r\r100\r\r50\r\r50\r\r50\r\r33\r\r\r\r\rThis second point is a critical issue: simply biasing the model without improving its accuracy should never produce different error values.\nTaking all this into account, the proposed metric seems to be even worse than the original one. Quite surprising!\n\rAn alternative\rSome others have proposed the log ratio \\(ln(F_t/A_t)\\) as a better alternative to MAPE.\rIt shows, indeed, better statistical properties than others metrics:\n\rApplying least squares regression to this metric estimates the geometric mean whereas minimizing MAPE or sMAPE does not lead to an established measure of location.\rGiven that geometric mean cannot exceed arithmetic mean, using least squares with log ratio will be more robust to outliers than OLS.\rIts values belong to a symmetric range: \\((-\\infty, \\infty)\\). However, it shows the same asymmetric behaviour than sMAPE: Exchanging actuals and forecasts holds the error\r(with a negative value!), but similar absolute deviations with same actual value don’t correspond with equal error values.\r\rIf interested in more information about this less-known metric, check out A better measure of relative prediction accuracy for model selection and model estimation (Chris Tofallis, 2015)\nI hope this may have brought some light into this quirky behaviour of MAPE and sMAPE.\nI also wrote a more general discussion about forecasting error metrics you might want to take a look at.\n\r","date":1566777600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566777600,"objectID":"fe907609b0810ca6ee3bda7c974310de","permalink":"/post/symmetric-mape-is-not-symmetric/","publishdate":"2019-08-26T00:00:00Z","relpermalink":"/post/symmetric-mape-is-not-symmetric/","section":"post","summary":"Against all odds, it isn't","tags":[],"title":"Symmetric MAPE is not symmetric","type":"post"},{"authors":["Carlos Vecina"],"categories":["R","Post"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rIn 7 minutes reading, You will learn how to turn your ggplot visualizations into amazing interactive 3D plots you can export or embed in HTML/Rmarkdown. Or even better, you will export as mp4 an animation rotating the figure.\nAs a use case, we are going to join the Spanish demographic data and GIS map, and then visualize it\n\n1. Introduction\r\nDuring the last weeks a ‘new’ package has received the R community attention. We say ‘new’ because it joined recently the CRAN, althought the very first commits in github repo date back more than a year. Its name is rayshader and in the author’s own words:\n\n\r“rayshader uses elevation data in a base R matrix and a combination of raytracing, spherical texture mapping, overlays, and ambient occlusion to generate beautiful topographic 2D and 3D maps”\n\r\nIn my view, Tyler Morgan-Wall (package’s author) hit the jackpot with the new addition of two specific functions. These are plot_gg() and render_movie(). The first one converts the ggplot to a 3D figure using one or two lines of code making it deadly-simple. The second one renders an animation in which we can set up several parameters like zoom, fps, angles and inclinations… as user-friendly as possible.\n\n\nLet’s try these new functionalities! \rThe only condition you must have a color or fill aesthetic, unless you can also play in the same plot wiht size.\rMany times 3D plots are not the right choice for most of the data visualization cases. Therefore, I tried to bring to this article a non gratuitous example.\nAs a practical challenge, we will visualize in an interactive 3D map the average age in each city of Spain. Cool? First of all we need the population stats. We get it from the INE webpage. Secondly we have to delimiter Spanish cities with they GIS coordinates. Then we are merging these data to create a ggplot chart. Once we have the ggplot object we are going to use the rayshader package to map color aesthetic to the third spatial dimension. To conclude, we are going to render it as rotating 3D video.\nLet´s do it step by step.\n\n\r\r2. Visualazing Spanish cities average age.\rWe usually want to start our pratical work drawing the main steps in our project and our principal goals. So in a general layer, we want to visualiza the average age. Firstly in a ggplot-color way, go one step further and make the plot 3D and end with an animation where the Z axis will be the average age.\n\n2.1- Downloading census data\rAs said, for our purpose, we need to collect data from two sources. We use INE open data portal to download census ages data by city. After a not very user-friendly search, we got it. I provide you the following link, where you can find the continuous register statistics:\rlink.\nAiming to keep focused, we don’t get distracted and we are going to download the 2018 file. However, is worth noting the INEbase efforts to make easier the INE open data platform.\nWe start loading (or downloading) the packages we are going to use. In other article or tip we will provide a custom function to Load and Download Rpackages in onle line.\rMoreover we define the required functions and download directories.\nlibrary(pxR)\rlibrary(RColorBrewer)\rlibrary(rgeos)\r#install.packages(\u0026quot;rgdal\u0026quot;, repos = \u0026quot;http://cran.us.r-project.org\u0026quot;) reinstall cause gpclib dependencie https://stackoverflow.com/questions/30790036/error-istruegpclibpermitstatus-is-not-true\rlibrary(rgdal)\rlibrary(rayshader)\rlibrary(knitr)\rlibrary(magrittr)\rlibrary(tidyverse)\r\ras.numeric.factor \u0026lt;-function(x) { # Custom function to convert fctr to num factor value\rreturn(suppressWarnings(as.numeric(levels(x))[x]))\r}\r\rif(!dir.exists(\u0026quot;data\u0026quot;)) dir.create(\u0026quot;data\u0026quot;) # Create the download directory\r\nDownloading INE 2018 file:\nutils::download.file(url = \u0026quot;http://www.ine.es/pcaxisdl/t20/e245/p05/a2018/l0/00000006.px\u0026quot;,\rdestfile = \u0026quot;data/census_2018.px\u0026quot;)\r\ntbl_census_2018 \u0026lt;-read.px(\u0026quot;data/census_2018.px\u0026quot;) %\u0026gt;%# Load \u0026amp; format\ras_tibble()\rWe parse the data to obtain a name,pcode,average age dataframe\ntbl_census_2018 %\u0026lt;\u0026gt;%\rset_names(c(\u0026quot;age\u0026quot;, \u0026quot;city\u0026quot;, \u0026quot;sex\u0026quot;, \u0026quot;population\u0026quot;)) %\u0026gt;%# Cambiamos los nombre\rna.omit() %\u0026gt;%# Na rmv\rfilter((city!=\u0026quot;Total\u0026quot;)\u0026amp;(age!=\u0026quot;Total\u0026quot;)\u0026amp;(sex==\u0026quot;Ambos sexos\u0026quot;)) %\u0026gt;%# Duplicate info rmv\rseparate(city, c(\u0026#39;postal_code\u0026#39;, \u0026#39;city_name\u0026#39;), sep=\u0026quot;-\u0026quot;) %\u0026gt;%# Sep City column\rmutate(age = as.numeric.factor(age)) %\u0026gt;%# Conv to numeric\rgroup_by(city_name, postal_code) %\u0026gt;%# Group to operate\rsummarise(avg_age = sum(population*age,na.rm = T)/sum(population,na.rm=T)) %\u0026gt;%# Avg age\rselect(city_name, postal_code, avg_age) # Discard columns\r\n\r2.2- Downloading GIS data\rThe second source we are going to use is the Geo data. We will use cities coordinates and matching it with Spanish demographic data previously obtained.\nDownloading map overlay:\ntemp \u0026lt;-tempfile() # Create the tempfile\ru=\u0026quot;http://www.arcgis.com/sharing/rest/content/items/8e31c4c1a0b348f79058f212d0d807a1/data\u0026quot;\rutils::download.file(url = u, destfile = temp,\rmode=\u0026quot;wb\u0026quot;) # Binary mode for correct download\r\runzip(temp, exdir = \u0026quot;data/cities_gis\u0026quot;) # Unzip in data/cities_gis\runlink(temp) # Delete temp file\rWe parse the spatial information to convert it into tabular data. We expect that the Canary Islands coordinates will skew the plot, so it’s our decision to keep focused in our 3D objetive and filter peninsular coordinates. It’s also possible, and a better practice, to move insular coordinates looking for a compact plot, instead of filter them out.\nTo complete this data processing, we use fortify function that allows us to don’t load more packages. However, this function throws a warning suggesting the broom::tidy() one.\ntlb_cities_gis \u0026lt;-readOGR(dsn = \u0026quot;./data/cities_gis/Municipios_ETRS89_30N.shp\u0026quot;,\rverbose=FALSE) # Spatial data reading\rtlb_cities_gis %\u0026lt;\u0026gt;%\rfortify(region = \u0026quot;Codigo\u0026quot;) # %\u0026gt;% # Conv \u0026quot;spatial object\u0026quot; to data.frame\r# broom::tidy()\r\rplot_canarias \u0026lt;-F # Control param, initial app config\r\rif(plot_canarias==F){ # Should be moduled in a funct\rtlb_cities_gis %\u0026lt;\u0026gt;%\rfilter((long\u0026gt;0) \u0026amp;(lat\u0026gt;4000000)) # Filter peninsular data\r} \rFinaly, we join both creating the final dataset, which we are going to use to make the plots. Note that we use left join to keep de geo data.\ntbl_cities_avg_age \u0026lt;-tlb_cities_gis %\u0026gt;%\rleft_join(tbl_census_2018, by = c(\u0026quot;id\u0026quot; =\u0026quot;postal_code\u0026quot;)) \rAs a good practice, we are going to check the number of NAs generated after the left join. These NAs meaning is that there are cities localized but without average year information\nWe can see that these missing values represents just 1% of the data, so we are going to impute them with the previous postal code info. I bet that you can easily improve this procedure but I consider it’s prety acceptable enought seeing the low NA ratio.\ntbl_cities_avg_age %\u0026gt;%\rgroup_by(id) %\u0026gt;%\rsummarise(na = sum(is.na(avg_age))) %\u0026gt;%# NAs by city\rsummarise(missing_perc = sum(na\u0026gt;0)/length(na)*100) %\u0026gt;%# Perc cities with at least 1 na \rselect(missing_perc)\r\r\rtbl_cities_avg_age %\u0026lt;\u0026gt;%\rarrange(id) %\u0026gt;%\rfill(avg_age, .direction = \u0026quot;down\u0026quot;) # Fill with the previous pc data.\r\n\r2.3- GGplot visualization\rInspired in http://blog.manugarri.com/making-a-beautiful-map-of-spain-in-ggplot2/\nOnce we have created the final dataset, we are able to start ploting it. Of course longitude in X-axis and latitude en Y-axis. Firstly average city age is represented using a color palette. Red colours are assigned to older people and blue ones to younger city population. We get it in ggplot with the fill aesthetic.\nmyPalette \u0026lt;-colorRampPalette(rev(brewer.pal(11, \u0026quot;Spectral\u0026quot;))) # Create reverse Spectral palette\r\rplot_cities \u0026lt;-ggplot() +\rgeom_polygon(data = tbl_cities_avg_age, aes(fill = avg_age, \rx = long, \ry = lat, \rgroup = id)) +# Dummy variable to correct fill by PCode.\rscale_fill_gradientn(colours=myPalette(4)) +# Choose palette colours.\rlabs(fill=\u0026quot;Avg age\u0026quot;)\rplot(plot_cities)\r\n\r2.4- 3D Rayshader Visualization!\rThat was pretty nice. It’s sure that you can reach the general propose to be able to locate inmediately older an younger zones. Although as we will disccuss in a future post, human eyes aren’t ready to distinguiss almost nothing but big color contrasts. What about complement color with a third dimension through z axis?\nLet’s see how it works\nplot_gg(plot_cities,multicore=TRUE,width=5,height=3,scale=310) # Plot_gg de rayshader\rrender_snapshot(filename = \u0026quot;3D_spain\u0026quot;)\r\nHmm you told something about render_movie()… What if we anime it?\n\n\r2.5- 3D animation with rayshader\rIn the last plot, it results the correct angle election as a key point. But what if we animate it with a rotating effect?\nThis is what the following function take cares on:\nrender_movie(\u0026quot;img/movie_spain.mp4\u0026quot;,frames = 720, fps=30,zoom=0.6,fov = 30)\rThis way you can achieve the header 3D rotating image!\n\nYou can see related posts on TypeThePipe\n\n\rp {\rword-spacing: 3px;\rtext-indent: 20px;\rtext-align: justify;\r}\r.page-subtitle {\rtext-align: left !important;\rtext-indent: 0px !important;\r}\r.card-text {\rtext-align: left !important;\rtext-indent: 0px !important;\r}\r\r\rwindow.dojoRequire([\"mojo/signup-forms/Loader\"], function(L) { L.start({\"baseUrl\":\"mc.us4.list-manage.com\",\"uuid\":\"91551f7ed29389a0de4f47665\",\"lid\":\"d95c503a48\",\"uniqueMethods\":true}) })\r\r\r","date":1564185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564185600,"objectID":"1fbb630d0742d27444b87b493de2eba9","permalink":"/post/ggplot-to-3d-in-r-with-rayshader/","publishdate":"2019-07-27T00:00:00Z","relpermalink":"/post/ggplot-to-3d-in-r-with-rayshader/","section":"post","summary":"Do you want to add a 3rd dimension to your R plot? In this post we show you how to turn your R ggplot to a 3D plot easily with Rayshader.","tags":[],"title":"Turn your GGplot to 3D animation. Awesome 2D to 3D plots in R with Rayshader","type":"post"}]