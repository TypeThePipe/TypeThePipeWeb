[{"authors":["carlos-vecina"],"categories":null,"content":"Carlos Vecina es un científico de datos con experienca en la aplicación del Machine Learning y de la Inteligencia Artificial con el objetivo de aportar gran valor a negocio en áreas como el CRM, el Marketing y en la compra-venta dentro del mercado eléctrico.\n","date":1582502400,"expirydate":-62135596800,"kind":"taxonomy","lang":"es","lastmod":1582502400,"objectID":"7f2ce30e6e7155580a08c73da392044a","permalink":"https://typethepipe.com/es/authors/carlos-vecina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/es/authors/carlos-vecina/","section":"authors","summary":"Carlos Vecina es un científico de datos con experienca en la aplicación del Machine Learning y de la Inteligencia Artificial con el objetivo de aportar gran valor a negocio en áreas como el CRM, el Marketing y en la compra-venta dentro del mercado eléctrico.","tags":null,"title":"Carlos Vecina","type":"authors"},{"authors":["pablo-canovas"],"categories":null,"content":"Pablo Cánovas es un científico de datos con experiencia desarrollando y productivizando modelos de Machine Learning aplicados al mercado energético europeo.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"es","lastmod":-62135596800,"objectID":"78b4973aee7e471f2427600fe9f47d84","permalink":"https://typethepipe.com/es/authors/pablo-canovas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/es/authors/pablo-canovas/","section":"authors","summary":"Pablo Cánovas es un científico de datos con experiencia desarrollando y productivizando modelos de Machine Learning aplicados al mercado energético europeo.","tags":null,"title":"Pablo Cánovas","type":"authors"},{"authors":["Carlos Vecina"],"categories":["Post","Datathon"],"content":"\r¿Pensando en inscribirte en un Datathon o competición de datos? ¡En este post os traemos 10 consejos sobre los puntos clave para conseguir grandes resultados!\nClaves:\n\r1. Establece tu objetivo y revisa los recursos(habilidades, tiempo, acceso a máquinas…) con los que cuentas.\r2. Elección del tema/industria/CdUso.\r3. Construye tu equipo de manera consciente. Usa herramientas para compartir código/archivos.\r4. Investiga previa y profundamente sobre el caso de uso en concreto.\r5. Análisis exploratorio de los datos. Recuerda que (usualmente) no son los únicos que puedes usar.\r6. Establece la estructura del proyecto y del código.\r7. Lanza los primeros modelos.\r8. Validación cruzada, OOS y backtesting.\r9. Interpretación y evaluación de resultados desde el punto de vista práctico y/o negocio.\r10. Escribe una memoria con puntos fuertes y puntos a mejorar detectados, que te sirvan de partida en el próximo reto.\r\r\n1. Establece tu objetivo y revisa los recursos (habilidades, tiempo, acceso a máquinas…) con los que cuentas.\nComenzamos por un punto esencial. Dependiendo de tu perfil y de la competición, deberás plantearte a priori qué pretendes conseguir participando en el reto. Puede ser aprender y mejorar en un determinado campo o tecnología, conseguir el premio, quedar entre los 3 primeros para incluirlo en tu CV o simplemente por diversión. Cualquiera que sea está bien.\nLo que recomendamos es evaluarlo de manera sincera, ya que será bastante el tiempo invertido y tener tu objetivo presente te ayudará en los momentos de menor motivación.\nPor ejemplo, si tu objetivo no es ganar la competición (el caso de participar en tu primer Kaggle) seguramente prefieras no entrar en una espiral de hiperoptimización de hiperparámetros para rascar decimales a la métrica de error, sino que quizá prefieras estudiar y aplicar nuevas técnicas o algoritmos, aprender sobre desarrollo y estructuración de un proyecto de Data Science o integrar fuentes complementarias de datos.\n\n2. Elección del tema/industria/CdUso.\nLigado con esta última idea, dado que vamos a dedicar muchos esfuerzos en este proyecto, ¿cómo no elegir un tema, campo o caso de uso que nos motive? Afortunadamente, desde muchas organizaciones y plataformas se están lanzando diferentes e interesantes competiciones. Kaggle es uno de los máximos exponentes de estas plataformas, pero también existen otras, las cuales presentamos aquí.\n\n3. Construye tu equipo de manera consciente. Usa herramientas para compartir código/archivos.\nDependiendo de los dos puntos anteriores, especialmente del primero, según tu objetivo deberás elegir a tu equipo de manera consecuente. Este es un tema que no se suele comentar, pero consideramos importante prestarle la debida atención.\nElige un compañero o amigo con el que te lleves realmente bien y os motivéis mutuamente si tu objetivo es aprender sobre un tema o tecnología nueva. Prioriza a alguien que comparta tus intereses en el caso que desees dedicarle tiempo a profundizar y masterizar un tema, tecnología o algoritmo muy concreto.\nSi el proyecto lo requiere, y pretendes optar al premio o a posiciones altas, te recomendamos elegir un compañero que complemente tus habilidades. En el caso de que no se requiera multidisciplinariedad, opta por aquellos que tengan un nivel similar al tuyo. Si es posible, ligeramente superior.\nEligiendo tu equipo de manera consciente lograrás minimizar tiranteces y conflictos de objetivos, maximizando el retorno del tiempo invertido.\n\n4. Investigación previa y profunda sobre el caso de uso en concreto.\rUna vez formado el equipo, os recomendamos comenzar a investigar el negocio o el contexto en el que se enmarca vuestro proyecto. Esto permitirá plantear mejor el trabajo, extraer un valor muy superior a los datos y os evitará iteraciones en el proceso diseñar-implementar-evaluar al partir de premisas más claras. De esta manera dejareis al lado una mala interpretación de conceptos básicos, lo cual puede ser letal.\nUn ejemplo. Si la competición trata sobre predecir el número de visitas o el número de contrataciones de determinados productos en un comercio online, en base a la navegación recogida por Google Analytics, debéis controlar el comportamiento de esta fuente de datos. Conocer el significado de tasa de rebote, las mecánicas desde que un usuario se conecta por primera vez y se le asigna una cookie hasta que convierte, borrado de cookies, diferentes casuísticas de registros nulos, bots, que la source (para conocer la fuente de donde viene el visitante) en caso de dudas se asigna como direct…\nSin esta serie de conocimientos será difícil craftear variables para conseguir un buen performance de los modelos, o lo que es más importante, cualquier resultado que obtengáis será malinterpretado. Por lo tanto, todo este background no es imprescindible para llegar a algún resultado, pero sí lo será para llegar a resultados potentes y realizar una participación de la que acabes orgullosos.\n\n5. Análisis exploratorio de los datos. Recuerda que (usualmente) no son los únicos que puedes usar.\nComo todo proyecto de datos, consta de una serie de etapas iterativas. Después de investigar sobre el contexto, echareis un primer vistazo a los datos. Si surgen dudas, de nuevo tendréis que investigar para resolverlas.\nAl margen de este recordatorio, la fase exploratoria se centra en conocer cada una de las variables. Como consejo personal, entre otras cosas, nosotros comenzamos midiendo el porcentaje de NAs y la distribución y la varianza de las variables. Aquellas que tengan un elevado porcentaje de NAs o una varianza muy pequeña, o bien las apartamos (podría replantearse su entrada en siguientes iteraciones) o bien las encodeamos de manera dicotómica Si_NA/No_NA o Mayoritaria/No_mayoritaria. Para otro tipo de encodings ver.\nEsta etapa exploratoria varía bastante dependiendo del número de variables que contenga vuestro dataset. En datasets con 75, 100 o más variables, resultará más complicado hacerse una idea general de las características de cada una de ellas. Enfrentarnos a este tipo de datasets puede resultar ciertamente abrumador, sin embargo tras una buen estudio, eleccion y transformación puede ser que acabéis dando las gracias de no tener sólo 5 o 10 variables(caso en el que se suele llegar a un plateau de desempeño de los modelos más rápido y mayor homogeneidad en los resultados expuestos por los diferentes equipos).\nAdemás de esto; análisis de correlaciones, distribución de variables, de outliers univariante y multivariante, tests estadísticos preliminares… son análisis que os permitirán poner en contexto los datos y modelarlos mejor.\nPor último, recordar que normalmente no hay problema con utilizar datos complementarios a los propios de la competición. En algunos casos como Kaggle, lo que se establece es la obligación de comentar y hacer público el uso de estos datos durante la competición.\nDatos demográficos que ayuden a poner en contexto las variables geográficas como código postal o provincia, aportar explicabilidad a spikes o eventos pasados… son sólo algunos de los ejemplos de los datos que se pueden incorporar al dataset original del reto, obviamente teniendo en cuenta cuales de estos datos vais a tener y cuales no a la hora de predecir.\n\n6. Establecer la estructura del proyecto y del código.\nA continuación os mostramos la estructura que suelen tener nuestros proyectos. La estructuración de los directorios dentro de un proyecto de Data Science dependerá de las características del entorno tanto de desarrollo como el la posterior productivización, siguiendo principios generales como la modularización del código.\rUna estructura bastante generalizada sería la siguiente:\nProjecto:\n\rdata:\r\r1_raw:\r2_processed:\r\rmodels:\rnotebooks:\r\r1_eda:\r2_poc:\r3_modeling:\r4_evaluation:\r\rsrc:\r\r1_get_data:\r2_processing:\r3_modeling:\r4_evaluation:\r5_helpers:\r\r\rCreemos que esta estructura se podría simplificar un poco dadas las características de un Datathon, en la que no se necesita automatizar la ingesta y el preprocesado de los datos, sino que será una tarea puntual. Además el análisis exploratorio aquí suele tener un papel protagonista, así como el modelado se suele simplificar en un main.exe que ejecute todo el programa llamando a los módulos de preprocesado, train, test y evaluación.\nProjecto_Datathon:\n\rdata:\rexploratory:\rhelpers:\rlog:\rmain.R / main.py\routputs:\r\rmodels:\rpreds:\rvalidation:\r\r\rEn cuanto a la estructura del main, solemos llevar a cabo una validación cruzada manual con el objetivo de ganar flexibilidad a la hora de usar modelos de diferente naturaleza y poder stackear sus predicciones y compararlos. Cierto es que soluciones paquetizadas como scikit-learn o H2O pueden hacer este trabajo en el caso de que el tiempo disponible y características de la competición indiquen su uso. Nuestra propuesta tendría este esqueleto:\nCarga de entorno (paquetes, módulos y funciones)\rDiferentes craft de variables\rSeparación en dataset y datasetOOSample\rSeparar el dataset en folds\rPor cada fold:\rEntrenar con el resto\rPredecir en el fold\rEvaluación\r(En el último fold, entrenar si se quiere modelos de stacking con las predicciones en trainSet de los anteriores modelos)\rPredicción OOSample\rEvaluación\rPredicción del conjunto de test a enviar.\n\n7. Lanzar los primeros modelos.\nEn primer lugar debéis centraros en una familia de algoritmos y función de coste que a priori mejor se adapten a los datos, al objetivo a predecir, incluso teniendo en cuenta la métrica de error por la que se os va a medir. Es importante sacar los primeros resultados, a ser posible dentro de una estructura similar a la propuesta anteriormente. Las primeras métricas de error os ayudaran en muchos aspectos. Por un lado os pueden dar pistas de bugs en el código (métricas de error no realistas). También os servirá de base a partir de la cual ir mejorando, y como estamos seguros de que lo haréis, esto será ademañs un boost de ánimo.\nOs invitamos a buscar en Google sobre las principales familias de algoritmos supervisados (suele ser el caso en estos Datathones) y de las principales funciones de coste a optimizar, dependiendo de si se trata de un problema de clasificación o de regresión.\nComentar que la participación en el foro y kernels de Kaggle, hilos de Reddit y videos en Youtube será un buen complemento a la lectura de papers.\n\n8. Validación cruzada, OOS y backtesting.\nEspecial mención a las métricas de error. Uno de los puntos a los que de manera personal más importancia le damos, es el conocer con la mayor precisión posible los intervalos de error(confianza o predicción) de nuestros modelos antes de enviarlos. Conocer las precisión de nuestro modelo bajo diferentes escenarios, partes del dataset e incluso datos sintéticos nos hace sentir un especial orgullo. Quizá sea simplemente un objetivo que nos marcamos(tal y como comentamos en el punto 1), pero el hecho de tener la certeza de cuando se baja la métrica de error no se debe a algo espúreo, o lo peor de todo a un bug, nos permite desarrollar con tranquilidad. Además en el caso de existir un ranking público y otro privado, controlar tu métrica de evaluación te permite saber con mayor certeza en que percentil te encuentras.\nLa mayoría de competiciones suelen ir ligadas a una métrica de error por la cual se evalúa a los participantes. Será fundamental introducirla en la evaluación de modelos, pero no debe de ser la única. Tanto a la hora de fijar una función de coste como a la hora de evaluar los modelos, deberíais tener en cuenta métricas de error complementarias que os ayuden a interpretar el desempeño de los modelos. Calcular el error mediano o el MAPE, cuando la métrica con la que os evaluarán será el error medio, puede ayudar tanto a conocer el comportamiento de vuestro modelo como a decantaros entre algoritmos que difieren en sólo mínimo porcentaje en la métrica principal en el conjunto de datos, pero no en otras.\n\n9. Interpretación y evaluación de resultados desde el punto de vista práctico y/o negocio.\rEn el caso de que se trate de un Datathon organizado por alguna empresa de vuestra zona, seguramente los mejores proyectos serán llamados a un evento final de presentación de los trabajos. En otros, os pedirán una memoria presentando los resultados. Si bien es cierto, en el caso de Kaggle no suelen hacerlo.\nPor lo tanto, si la competición a la que te/os presentáis requiere de este tipo de resultado, será importante tenerlo presente no sólo al final, sino durante todo el proceso. Una vez que se ha re-optimizado los hiperparámetros con la última librería bayesiana y stackeado nuestros mejores modelos, quizá sea hora de trabajar en la interpretabilidad de los resultados e incorporación de nuevos datos. En definitiva, conclusiones que sean valiosas a la hora de poner en marcha el modelo y que no quede en una simple matriz de pesos o de reglas.\nEn la última competición a la que asistimos se nos pedía esta presentación final a los tres mejores proyectos. Optamos por adentrarnos en el modelado de unas imágenes que se nos aportaban mediante transfer learning, cosa que no se nos pedía explícitamente. Una vez que modelamos la información estructurada y llevamos a cabo ciertas iteraciones sobre estos datos, pensamos que sería más enriquecedor tanto para nosotros como para las personas que proponían el concurso, la investigación sobre la extracción de información de las imágenes (información que en el caso de uso que nos ocupaba, a priori, iba a tener un impacto claro sobre la variable a predecir).\n\n10. Escribir una memoria con puntos fuertes y puntos a mejorar detectados, que te sirvan de partida en el próximo reto.\nUna vez finalizado el proyecto, un práctica que nos ha enriquecido y ha supuesto un antes y un después ha sido escribir algún tipo de memoria o documento para nosotros mismos. En él reflexionamos sobre la reciente participación. Analizamos los puntos fuertes y lo que debemos mejorar, tanto a nivel técnico como a nivel de equipo y por qué no a nivel emocional y de actitud. A ser posible nada más entregar y antes de saber el resultado. Sería genial si al tiempo de conocer el mismo se añadiera algún punto o reflexión más.\nTodas estas anotaciones os ayudarán, cada vez que os enfrentéis a un nuevo proyecto, a encararlo más preparados y no depender del ‘instinto’ ni de lo que la memoria selectiva os quiera recordar en momentos puntuales.\nUna mala predisposición personal, no investigar suficientemente y con cariño la distribución de los datos y sus valores extremos, una mala elección de equipo, seguir corriendo los modelos 30 minutos antes de que acabe el plazo de entrega, no preparar la exposición final con tiempo,… son situaciones que nos han pasado a muchos y no ocurre absolutamente nada por reflexionar sobre ello y dejarlo por escrito. Lo fundamental es, dentro de lo posible, ¡que no se repitan en la próxima competición para lograr conseguir aun mejores resultados! :)\n\rbody {\rtext-align: justify}\rp {\rword-spacing: 3px;\rtext-indent: 20px;\r}\r\r\rwindow.dojoRequire([\"mojo/signup-forms/Loader\"], function(L) { L.start({\"baseUrl\":\"mc.us4.list-manage.com\",\"uuid\":\"91551f7ed29389a0de4f47665\",\"lid\":\"d95c503a48\",\"uniqueMethods\":true}) })\r","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1582502400,"objectID":"0dcbc6d97d0dd73827a66fdcde3c0dde","permalink":"https://typethepipe.com/es/post/10-consejos-datathon/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/es/post/10-consejos-datathon/","section":"post","summary":"¿Quieres conseguir tus objetivos?","tags":[],"title":"10 consejos para triunfar en tu próximo Datathon","type":"post"},{"authors":["Carlos Vecina"],"categories":["R","Post","Shiny","Ggplot"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rAquí os mostramps el código de R. Como podeis ver, solo cargamos shiny, ggplot2 y tidyverse. Es un script en curso, ya que aun queda por desarrollar características básicas como:\n\rrefactorizar el código ;\n\rvisualización con GGanimate de los pasos recorridos por los algoritmos;\n\ry, por supuesto, implementar diferentes algoritmos pathfinding y evolutivos.\n\r\r¡Todas estas nuevos desarrollos y mucho más en siguientes posts! ¡Sigue informado!\nlibrary (shiny)\rlibrary (ggplot2)\rlibrary (tidyverse)\rsource(\u0026quot;helpers/ColourBorders.R\u0026quot;)\rsource(\u0026quot;helpers/PlotMapGrid.R\u0026quot;)\r\r\rui \u0026lt;-fluidPage(\rmainPanel(\rcolumn(12,offset = 5, \rtitlePanel(\u0026quot;Pathfinding Algorithm Visualization using R!\u0026quot;)),\rHTML(\u0026quot;\u0026amp;nbsp\u0026quot;),\rcolumn(12,offset = 5,HTML(\u0026quot;\u0026amp;nbsp\u0026quot;),\ractionButton(\u0026quot;go_search_actionButton\u0026quot;, \u0026quot;Go Search!\u0026quot;),\ractionButton(\u0026quot;clean_all_actionButton\u0026quot;, \u0026quot;Clean All\u0026quot;)),\rHTML(\u0026quot;\u0026amp;nbsp\u0026quot;),\rcolumn(12,offset=5, plotOutput(\u0026quot;map_grid_plotOutput\u0026quot;,\rclick=\u0026quot;map_grid_plotOutput_click\u0026quot;))\r))\r\r\rserver \u0026lt;-function(input, output){\r\r## Initial params\rmax_steps \u0026lt;-50\rmatrix_x_size \u0026lt;-20\rmatrix_y_size \u0026lt;-20\rgrid_map_reactive \u0026lt;-matrix(ncol = matrix_x_size,\rnrow = matrix_y_size,\rdata = 0) \r\r## Colours Dict (in progress)\r# 1- Wall\r# 2- Init\r# 3- Obj\r# 4- Step done\r# 5- Goal achieved\r\r# Initialize objts\rgrid_map_reactive[4,15] \u0026lt;-3 # obj\rgrid_map_reactive[17,3] \u0026lt;-2 # init\rinitial_step \u0026lt;-which(grid_map_reactive ==2,\rarr.ind = TRUE)\rgrid_map_reactive \u0026lt;-ColourBorders(grid_map_reactive, 1) # rounding walls\rreact_df \u0026lt;-reactiveValues(df = grid_map_reactive,\rorig = grid_map_reactive,\rwalls = grid_map_reactive)\r\robserve({\rif(!is.null(input$map_grid_plotOutput_click)){\rnew_x_value \u0026lt;-trunc(input$map_grid_plotOutput_click$x)\rnew_y_value \u0026lt;-trunc(input$map_grid_plotOutput_click$y)\r\rif(between(new_x_value,2,matrix_x_size-1) \u0026amp;between(new_y_value,2,matrix_y_size-1)){\risolate(react_df$df[new_y_value,new_x_value] \u0026lt;-if_else(react_df$df[new_y_value,new_x_value]==0,\r1,0))\risolate(react_df$df[4,15] \u0026lt;-3)\risolate(react_df$df[17,3] \u0026lt;-2)\risolate(react_df$df[17,3] \u0026lt;-2)\risolate(react_df$walls \u0026lt;-react_df$df)\r\routput$map_grid_plotOutput \u0026lt;-renderPlot({\r\rPlotMapGrid(react_df$df,\rmatrix_x_size,\rmatrix_y_size)\r\r}, width=600, height=600,position=\u0026quot;center\u0026quot;)\r}}\r}) \r\r# Go search! Pseudo-random pathfinding algortihm\robserveEvent(input$go_search_actionButton,{\r\rif(nrow(which(react_df$df ==4, arr.ind = TRUE))\u0026gt;=1) react_df$df \u0026lt;-react_df$walls # click search without clean\rcurrent_step \u0026lt;-initial_step \robj \u0026lt;-which(react_df$df ==3, arr.ind = TRUE)\rprevious_steps_with_opt \u0026lt;-current_step\r\rfor(i in 1:max_steps){\rnext_step_col \u0026lt;-tribble(~row, ~col,\rcurrent_step[1]+1,current_step[2]+0,\rcurrent_step[1]+0,current_step[2]+1,\rcurrent_step[1]-1,current_step[2]+0,\rcurrent_step[1]+0,current_step[2]-1)\rnext_values \u0026lt;-NULL\r\rfor(r in 1:nrow(next_step_col)){\rnext_values \u0026lt;-c(next_values,\rreact_df$df[next_step_col[[r,1]],\rnext_step_col[[r,2]]])\r}\rif(3 %in%next_values){\r\rcurrent_step \u0026lt;-next_step_col[next_values==3,] %\u0026gt;%\ras.matrix()\r\rreact_df$df[current_step] \u0026lt;-5\r\rbreak()\r\r} else if(0 %in%next_values){\r\rif(sum(next_values==0)\u0026gt;1){\r\rprevious_steps_with_opt \u0026lt;-current_step\r\r}\r\rcurrent_step \u0026lt;-next_step_col[next_values==0,] %\u0026gt;%\rsample_n(1) %\u0026gt;%\ras.matrix()\r\rreact_df$df[current_step] \u0026lt;-4\r\r} else {\r\rcurrent_step \u0026lt;-previous_steps_with_opt\r\r}\r}\r})\r\r# Reset all\robserveEvent(input$clean_all_actionButton,{\r\rreact_df$df \u0026lt;-react_df$orig\rreact_df$walls \u0026lt;-react_df$orig\r\r})\r\r# First panel\routput$map_grid_plotOutput \u0026lt;-renderPlot({\r\rPlotMapGrid(react_df$df,\rmatrix_x_size,\rmatrix_y_size)\r\r}, width=550, height=600,position=\u0026quot;center\u0026quot;)\r\r}\r\r\rshinyApp(ui=ui, server = server)\rPara acabar, las funciones helpers:\nColourBorders \u0026lt;-function(df, col_value){\r\r## Rounding walls \r# Params: df - Map grid\r# col_value - Colour to fill the rounding blocks\r# Return: df with the filled roundings\r\rdf[1,] \u0026lt;-col_value\rdf[,1] \u0026lt;-col_value\rdf[nrow(df),] \u0026lt;-col_value\rdf[,ncol(df)] \u0026lt;-col_value\r\rreturn(df)\r\r}\r\rPlotMapGrid \u0026lt;-function(df, matrix_x_size, matrix_y_size){\r\r## Plot the interactive grid \r# Params: df - Map grid\r# matrix_x_size - X_axis limit\r# matrix_y_size - Y_axis limit\r# Return: plot with the pathfinding\r\r\rplot \u0026lt;-rbind(\rwhich(df==1, arr.ind = TRUE) %\u0026gt;%cbind(fill_col=\u0026quot;#623B17\u0026quot;),\rwhich(df ==2, arr.ind = TRUE) %\u0026gt;%cbind(fill_col=\u0026quot;#13293D\u0026quot;),\rwhich(df ==3, arr.ind = TRUE) %\u0026gt;%cbind(fill_col=\u0026quot;#ffff66\u0026quot;),\rwhich(df ==4, arr.ind = TRUE) %\u0026gt;%cbind(fill_col=\u0026quot;#99ccff\u0026quot;),\rwhich(df ==5, arr.ind = TRUE) %\u0026gt;%cbind(fill_col=\u0026quot;#1B998B\u0026quot;)\r\r) %\u0026gt;%\rdata.frame(stringsAsFactors = F) %\u0026gt;%\rtransmute(y = as.numeric(row), x = as.numeric(col), fill_col=fill_col) %\u0026gt;%\rggplot(aes(x+0.5,y+0.5)) +\rgeom_tile(width = 1, height = 1, fill = df$fill_col, col=\u0026quot;black\u0026quot;) +\rscale_y_reverse() +\rscale_x_continuous(breaks = seq(0, matrix_x_size, 1),\rlimits = c(0+0.5, matrix_x_size+1.5), \rminor_breaks = NULL) +\rscale_y_continuous(breaks = seq(0, matrix_y_size, 1),\rlimits = c(0+0.5, matrix_y_size+1.5),\rminor_breaks = NULL) +\rtheme_linedraw()+\rtheme(axis.title.x=element_blank(),\raxis.title.y=element_blank(),\raxis.text.x=element_blank(),\raxis.text.y=element_blank(),\raxis.ticks.x=element_blank(),\raxis.ticks.y=element_blank())\r\rreturn(plot)\r\r}\r\rbody {\rtext-align: justify}\rp {\rword-spacing: 3px;\r}\r\r\rwindow.dojoRequire([\"mojo/signup-forms/Loader\"], function(L) { L.start({\"baseUrl\":\"mc.us4.list-manage.com\",\"uuid\":\"91551f7ed29389a0de4f47665\",\"lid\":\"d95c503a48\",\"uniqueMethods\":true}) })\r","date":1579564800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1579564800,"objectID":"9e2689accd5ab535c793e5936e7b81cd","permalink":"https://typethepipe.com/es/post/visualizador-algoritmos-busqueda-caminos/","publishdate":"2020-01-21T00:00:00Z","relpermalink":"/es/post/visualizador-algoritmos-busqueda-caminos/","section":"post","summary":"Diseñando el mapa interactivo para visualizar algoritmos evolutivos y de búsqueda de caminos.","tags":[],"title":"Visualizador de algoritmos en R! (I) Desarrollando el mapa dinámico","type":"post"},{"authors":["Carlos Vecina"],"categories":["R","Tips","Tidyverse"],"content":"\r\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rImagina que estas analizando datos obtenidos en encuestas. Quizá no hace falta que imagines y es realmente tu caso. Cuando tratamos de sacar estadísiticos de la distribución que siguen las respuestas, como la media, solemos hacerlo de una manera representativa, es decir, ponderando las respuestas dependiendo del número de individuos pertenecientes a un segmento.\nEs cierto que en R se encuentra la función weigthed.mean() y puede ser util en determinados casos. Pero, ¿y qué pasa si aun no tienes calculados los pesos y almacenados como vector o columna? Incluso más importante, ¿qué pasa si quieres utilizar otra función de agregación que no sea la media?\nAquí ofrecemos nuestra versión donde conseguimos obtener tanto los pesos como la agregación(de manera flexible, media u otra función) de manera ponderada:\nlibrary(tidyverse)\r\rsurvey_data \u0026lt;-tribble( # Creamos el dataset\r~id, ~region1, ~region2, ~gender, ~q1, ~q2,\r1,\u0026quot;sp\u0026quot;,\u0026quot;mad\u0026quot;,\u0026quot;m\u0026quot;, 2,5,\r2,\u0026quot;it\u0026quot;, \u0026quot;bol\u0026quot;, \u0026quot;m\u0026quot;, 5, 10,\r3,\u0026quot;sp\u0026quot;, \u0026quot;bar\u0026quot;, \u0026quot;f\u0026quot;, 2, 2,\r4,\u0026quot;sp\u0026quot;, \u0026quot;bar\u0026quot;, \u0026quot;f\u0026quot;, 7, 7,\r5,\u0026quot;it\u0026quot;, \u0026quot;bol\u0026quot;, \u0026quot;m\u0026quot;, 2, 7) \rsurvey_data %\u0026gt;%\rgroup_by(region1, region2, gender) %\u0026gt;%# Elegimos nuestros segmentos\rmutate(weight = 1/n()) %\u0026gt;%# Calculamos los pesos\rungroup() %\u0026gt;%# Una vez calculados desagrupamos\rsummarise_at(vars(contains(\u0026quot;q\u0026quot;)), # Son preguntas columnas que empiezan por q\rfuns(weighted_mean = # Elegimos como funcion de agregación la media\rsum(. *weight)/sum(weight))) \r\r\rq1_weighted_mean\r\rq2_weighted_mean\r\r\r\r\r\r3.333333\r\r6\r\r\r\r\r","date":1579132800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1579132800,"objectID":"012435e267765e93675d8ea0f583660f","permalink":"https://typethepipe.com/es/vizs-and-tips/summarise_at-media-ponderada-tidyverse/","publishdate":"2020-01-16T00:00:00Z","relpermalink":"/es/vizs-and-tips/summarise_at-media-ponderada-tidyverse/","section":"vizs-and-tips","summary":"Analizando datos de encuestas usando R.","tags":[],"title":"Dominando la función summarise_at(). Media ponderada con R en el ecosistema Tidyverse.","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["Python","Tips","Magic"],"content":"\rUna queja o comentario recurrente entre los usuarios que empiezan a usar los Jupyter Notebooks es la falta de informacion sobre las variables y funciones definidas en el entorno. Si te realizas esta pregunta, seguramente necesites informacion sobre el uso y el principal objetivo de estos notebooks, el cual es muy diferente al de IDEs como Spyder, Pycharm o RStudio.\nUna vez que confirmamos que este tipo de notebooks son lo que necesitamos, existen diferentes maneras de mostrar esta información. La primera y más facil de ellas es mediante el método mágico %whos\nOtras alternativas son, por un lado la extensión nbextension y dentro de Jupyter Lab el inspector de variables. Puedes encontrar más información aquí.\n","date":1578960000,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1578960000,"objectID":"28ebdcebc3be362d7f68c200d4adcfcb","permalink":"https://typethepipe.com/es/vizs-and-tips/magic-whos/","publishdate":"2020-01-14T00:00:00Z","relpermalink":"/es/vizs-and-tips/magic-whos/","section":"vizs-and-tips","summary":"Kind of magic","tags":[],"title":"Lista todas las variables definidas en Jupyter Notebooks","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["R","Vizs","Ggplot2"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\rA la hora de presentar tus habilidades ya sea en una página web o en tu CV, una buena idea sueles ser sintetizarlas mediante una gráfica. Muchas plantillas de CVs incluyen una gráfica por defecto.\nEn este post queremos dar un template el cual poder personalizar con vuestras habilidades y evolucionar incorporando mejoras estéticas. ¡Estaremos encantados de ver tu versión!\nOs dejamos el código comentado en el siguiente chunk:\nlibrary(ggplot2)\r# library(plotly) Es simple transformarlo a plotly\rlibrary(tibble)\rlibrary(dplyr)\r\rskills \u0026lt;-tribble( # Creamos el dataset con las habilidades\r~Skill, ~Hours, ~Class,\r\u0026quot;AWS\u0026quot;, 500, \u0026quot;BigData\u0026quot;,\r\u0026quot;Python\u0026quot;, 8000, \u0026quot;Language\u0026quot;,\r\u0026quot;Spark\u0026quot;, 4000, \u0026quot;BigData\u0026quot;,\r\u0026quot;R\u0026quot;, 9000, \u0026quot;Language\u0026quot;,\r\u0026quot;Git\u0026quot;, 2000, \u0026quot;Tools\u0026quot;,\r\u0026quot;Jira\u0026quot;, 2000, \u0026quot;Tools\u0026quot;,\r\u0026quot;Forecasting\u0026quot;, 5000, \u0026quot;Objetive\u0026quot;,\r\u0026quot;Segmentation\u0026quot;, 2000, \u0026quot;Objetive\u0026quot;,\r\u0026quot;Computer Vision\u0026quot;, 600, \u0026quot;Objetive\u0026quot;,\r\u0026quot;SQL\u0026quot;, 4500, \u0026quot;Language\u0026quot;,\r\u0026quot;IBM Data Stage \u0026amp; SPSS\u0026quot;, 1200, \u0026quot;Tools\u0026quot;,\r\u0026quot;Shiny R\u0026quot;, 1500, \u0026quot;Visualization\u0026quot;,\r\u0026quot;Tableau\u0026quot;, 1000, \u0026quot;Visualization\u0026quot;,\r\u0026quot;Spotfire\u0026quot;, 500, \u0026quot;Visualization\u0026quot;\r) \r# plotly( \rggplot(data=skills,\raes(x=reorder(Skill,-desc(Hours)), # Ordenamos las habilidades según las horas dedicadas\ry= Hours, \rfill=Class, # Coloreamos según ek tipo de habilidad\rlabel=paste0(Hours,\u0026quot; h\u0026quot;))) +# Añadimos un label con las horas\rgeom_bar(stat = \u0026quot;identity\u0026quot;, colour=\u0026quot;black\u0026quot;) +# Stat identity para que pueda tener eje Y\rcoord_flip() +# Hacemos las barras horizontales\rlabs(x=\u0026quot; \u0026quot;, y=\u0026quot;Hours\u0026quot;, fill=\u0026quot; \u0026quot;) +# Definimos el nombre de los ejes\rtheme_minimal() +# Theme sin background\rscale_fill_brewer(palette = \u0026quot;YlOrBr\u0026quot;, # Paleta deseado\rdirection = -1) +\rgeom_label(show_guide = F, aes(y=400)) # Usar show_guide a pesar del warning\r","date":1578355200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1578355200,"objectID":"928907531386dec428888f171d0aca3c","permalink":"https://typethepipe.com/es/vizs-and-tips/skills-chart-curriculum/","publishdate":"2020-01-07T00:00:00Z","relpermalink":"/es/vizs-and-tips/skills-chart-curriculum/","section":"vizs-and-tips","summary":"Gráfico de habilidades con 2 lineas de Ggplot R.","tags":[],"title":"Muestra tus habilidades o *skills mediante un gráfico en R y su paquete Ggplot","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["R","Tips","Ggplot"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\r\nGGanimate es una libraría de R que está generando bastantes posts y comentarios debido al potencial que introduce en el aspecto de visualizaciones en el ecosistema R.\nIntentando aplicar esta librería a gráficos de barras y suevolución temporal, nos encontramos usuarios preguntando la posibilidad de que las barras no solo aumenten y disminuyan a lo largo del tiempo(de la animación) sino que también el orden de las mismas variara en función si eran mayores o menores y no permaneciesen en el orden del primer timestamp, en definitiva que se reordenaran.\nPor tanto, compartimos el siguiente código en el que las barras se reordenan a lo largo del tiempo. Es más en este ejemplo, tenemos 5 ciudades que entran y salen del gráfico en función de su valor durante el tiempo y visualizando el top 4.\nOs dejamos el código comentado:\nlibrary(ggplot2)\rlibrary(gganimate)\rlibrary(tidyverse)\rdf_evolution_data \u0026lt;-data.frame(Name=rep(c(\u0026quot;Madrid\u0026quot;,\u0026quot;Barcelona\u0026quot;, # Creamos el dataset\r\u0026quot;Valencia\u0026quot;,\u0026quot;Alicante\u0026quot;,\r\u0026quot;Sevilla\u0026quot;),5),\rYear = factor(sort(rep(2001:2005, 5))),\rValue = runif(25,100,1000))\rdf_evolution_data_filtered \u0026lt;-df_evolution_data %\u0026gt;%\rgroup_by(Year) %\u0026gt;%\rmutate(Rank = rank(Value)) %\u0026gt;%# Añadimos la columna rank y\rfilter(Rank \u0026gt;=2) # descartamos el de menor valor\rggplot(df_evolution_data_filtered) +\rgeom_col(aes(x=Rank, # Creamos el gráfico de barras\ry=Value,\rgroup=Name, # Afrumando y filleando por ciudad\rfill=Name),\rwidth=0.4) +\rgeom_text(aes(x=Rank, # Etiquetamos las barras con los nombres\ry=0,\rlabel=Name,\rgroup=Name),\rhjust=1.25) +\rtheme_minimal() +# Elegimos un theme que no sea gris\rylab(\u0026#39;Value\u0026#39;) +\rtheme(axis.title.y = element_blank(), # Eliminamos los labels y titles \raxis.text.y = element_blank(),\raxis.ticks.y = element_blank(),\rplot.margin = unit(c(5,5,5,5), \u0026#39;lines\u0026#39;)) +# Escogemos el zoom\rscale_fill_brewer(palette=\u0026quot;Dark2\u0026quot;) +# Paleta de colores de las barras\rcoord_flip(clip=\u0026#39;off\u0026#39;) +# Hacemos las barras horizontales\rggtitle(\u0026#39;{closest_state}\u0026#39;) +# Tilulo == al valor de la columna que\rtransition_states(Year, # Animamos la columna Year\rtransition_length = 1, # Duración de la animación de transición\rstate_length = 1) +# Duración de cada Year\rexit_fly(x_loc = 0, y_loc = 0) +# Salida de la ciudad no top4\renter_fly(x_loc = 0, y_loc = 0) # Entrada de la ciudad al top4\r","date":1576540800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1576540800,"objectID":"90ec1d6400b13e773818b1e133f3e258","permalink":"https://typethepipe.com/es/vizs-and-tips/reordenar-grafico-barras-gganimate/","publishdate":"2019-12-17T00:00:00Z","relpermalink":"/es/vizs-and-tips/reordenar-grafico-barras-gganimate/","section":"vizs-and-tips","summary":"Mostramos como reordenar las barras en las animaciones de ggplot realizadas con gganimate.","tags":[],"title":"Reordenando gráficos de barras en GGanimate","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["R","Tips","Ggplot"],"content":"\ra.sourceLine { display: inline-block; line-height: 1.25; }\ra.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }\ra.sourceLine:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode { white-space: pre; position: relative; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rcode.sourceCode { white-space: pre-wrap; }\ra.sourceLine { text-indent: -1em; padding-left: 1em; }\r}\rpre.numberSource a.sourceLine\r{ position: relative; left: -4em; }\rpre.numberSource a.sourceLine::before\r{ content: attr(title);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; pointer-events: all; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rcolor: #aaaaaa;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }\rdiv.sourceCode\r{ background-color: #f8f8f8; }\r@media screen {\ra.sourceLine::before { text-decoration: underline; }\r}\rcode span.al { color: #ef2929; } /* Alert */\rcode span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */\rcode span.at { color: #c4a000; } /* Attribute */\rcode span.bn { color: #0000cf; } /* BaseN */\rcode span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #4e9a06; } /* Char */\rcode span.cn { color: #000000; } /* Constant */\rcode span.co { color: #8f5902; font-style: italic; } /* Comment */\rcode span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */\rcode span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */\rcode span.dt { color: #204a87; } /* DataType */\rcode span.dv { color: #0000cf; } /* DecVal */\rcode span.er { color: #a40000; font-weight: bold; } /* Error */\rcode span.ex { } /* Extension */\rcode span.fl { color: #0000cf; } /* Float */\rcode span.fu { color: #000000; } /* Function */\rcode span.im { } /* Import */\rcode span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */\rcode span.kw { color: #204a87; font-weight: bold; } /* Keyword */\rcode span.op { color: #ce5c00; font-weight: bold; } /* Operator */\rcode span.ot { color: #8f5902; } /* Other */\rcode span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */\rcode span.sc { color: #000000; } /* SpecialChar */\rcode span.ss { color: #4e9a06; } /* SpecialString */\rcode span.st { color: #4e9a06; } /* String */\rcode span.va { color: #000000; } /* Variable */\rcode span.vs { color: #4e9a06; } /* VerbatimString */\rcode span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */\r\r\nAquí mostramos el código comentado.\nlibrary(tidyverse)\rhuron \u0026lt;-data.frame(year = 1875:1972, # Creamos el conjunto de datos\rvalue = LakeHuron,\rstd = runif(length(LakeHuron),0,1)) # Valores random para representar la desviación \r\rhuron %\u0026gt;%# Fijamos los ejes X e Y\rggplot(aes(year, value)) +\rgeom_line(color = \u0026quot;firebrick\u0026quot;, size = 1) +# Creamos la gráfica de linea \rgeom_ribbon(aes(ymin = value -std,\rymax = value +std), # Creamos los intervalos con valor +- std\rfill = \u0026quot;steelblue2\u0026quot;) \rPara un grafico con varias lineas, debemos incorporar el aesthetic de group de la siguiente manera:\nlibrary(tidyverse)\rhuron \u0026lt;-data.frame(year = rep(1875:1972,2), \rgroup = c(rep(\u0026quot;a\u0026quot;,98),rep(\u0026quot;b\u0026quot;,98)),\rvalue = c(LakeHuron, LakeHuron +5),\rstd = runif(length(LakeHuron)*2,0,1)) # Creamos el conjunto de datos con dos series\r\r# Seguimos la misma estructura para crear el plot, pero añadiendo fill=group y group=group para indicar que\r# coloree y trate las dos series de manera diferenciada.\rhuron %\u0026gt;%\rggplot(aes(year, value, fill = group)) +\rgeom_line(color = \u0026quot;firebrick\u0026quot;, size = 1) +\rgeom_ribbon(aes(ymin = value -std,\rymax = value +std,\rgroup=group),\rfill = \u0026quot;steelblue2\u0026quot;) \r","date":1574035200,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1574035200,"objectID":"ea3799e1edee098e2153caf2e4b83d01","permalink":"https://typethepipe.com/es/vizs-and-tips/ggplot_geom_ribbon_shadow/","publishdate":"2019-11-18T00:00:00Z","relpermalink":"/es/vizs-and-tips/ggplot_geom_ribbon_shadow/","section":"vizs-and-tips","summary":"¡Plotea tus intervalos de confianza de manera muy facil!","tags":[],"title":"Muestra los intervalos de confianza de tus predicciones mediante geom_ribbon() con Ggplot","type":"vizs-and-tips"},{"authors":["Carlos Vecina"],"categories":["Post","R"],"content":"\rEn 7 minutos, seremos capaces de convertir nuestras gráficas generadas con ggplot en espectaculares plots en 3D, ¡y ademas interactivos!\rPodrás embebernos en HTML/Rmarkdown, o incluso mejor, podras exportarlo como mp4 en una animacion rotatoria para sacarle todo el jugo a tus datos!\nComo caso de uso, vamos a visualizar la edad media de los municipios españoles cruzando datos del padrón con los datos GIS, para posteriormente visualizarlos en 3 dimensiones.\n\n1. Introducción\r\nEn las últimas semanas, un ‘nuevo’ paquete de R ha centrado el interés de cierta parte de la comunidad. Decimos ‘nuevo’ porque se ha incorporado recientemente al CRAN, aunque realmente el primer commit realizado por su autor su repo de Github data de hace más de un año. Su nombre es rayshader y en palabras de su propio creador:\n\n\r“rayshader uses elevation data in a base R matrix and a combination of raytracing, spherical texture mapping, overlays, and ambient occlusion to generate beautiful topographic 2D and 3D maps”\n\r\nBajo mi punto de vista, Tyler Morgan-Wall (el autor del paquete) dio con la tecla cuando incorporó al paquete dos nuevas funciones, plot_gg() y render_movie(). La primera de ellas nos permite convertir con 2 líneas de código nuestra visualización en ggplot a una figura 3D de una manera realmente facil y eficiente. La segunda de ellas renderiza esta figura y la anima, poniendo al alcance del usuario diversos parámetros para controlar el zoom, los fps, ángulo, inclinación…\n\n\nLas nuevas funcionalidades y planteamiento del experimento\rLa única condicion que debe cunplir tu gg-visualización es tener como aesthetic color o fill, y en algunos casos también puedes jugar con el size\nEn demasiadas ocasiones, la visualización de datos en 3D no es la mejor opción a elegir, tal y como hablaremos un un futuro post. Por este motivo, he intentado traer un ejemplo donde el uso de la tercera dimensión aporte valor al análisis.\nEste ejemplo práctico consistirá, como ya hemos avanzado, en la visualización en el mapa de España la edad media en cada municipio. ¿Chulo? Para ello necesitaremos:\n\rLos datos del censo sobre las estadísticas de la población (en este caso la edad media) por cada municipio. Estos datos los obtenemos de la web del INE.\n\rLos datos GIS con las coordenadas de cada uno de los municipios que componen España.\n\r\rUna vez que tengamos estas dos fuentes de datos combinados, los visualizaremos y posteriormente exploraremos su renderización en un clip 3D con la figura rotando tal y como se ve en la imagen que encabeza este post.\n¡Vayamos paso por paso!\n\n\r\r2. CdU: Visualizando la edad media de cada municipio en España\rUna vez hemos establecido el objetivo principal y las diferentes fuentes de los datos, podemos proceder a la descarga y tratamiento de los mismos.\n\n2.1- Descargando los datos del censo\rComo digimos, para llevar a cabo nuestro propósito, necesitamos acceder a dos fuentes diferentes de datos. Usaremos el portal de datos abiertos del INE para descargar la edad media en cada municipio español. Después de una búsqueda bastante ardua por su web, encontramos la información que buscábamos. Os dejo este link donde teneis acceso a lo que ellos llaman como estadísticas contínuas\nlink.\nCon el objetivo de no irnos por las ramas, descargaremos directamente el fichero del 2018. Sin embargo, sí que es interesante citar la iniciativa INEbase de facilitar el acceso y la navegación en esta fuente de datos de INE.\nComenzamos cargando (o descargando) los paquetes necesarios para nuestro análisis. En un futuro post o tip compartiremos una función nuestra para la carga (o descarga en caso necesario) múltiple de paquetes en una sola linea.\nlibrary(pxR)\rlibrary(RColorBrewer)\rlibrary(rgeos)\r#install.packages(\u0026quot;rgdal\u0026quot;, repos = \u0026quot;http://cran.us.r-project.org\u0026quot;) reinstall cause gpclib dependencie https://stackoverflow.com/questions/30790036/error-istruegpclibpermitstatus-is-not-true\rlibrary(rgdal)\rlibrary(rayshader)\rlibrary(knitr)\rlibrary(magrittr)\rlibrary(tidyverse)\ras.numeric.factor \u0026lt;- function(x) { # Custom function to convert fctr to num factor value\rreturn(suppressWarnings(as.numeric(levels(x))[x]))\r}\rif(!dir.exists(\u0026quot;data\u0026quot;)) dir.create(\u0026quot;data\u0026quot;) # Create the download directory\r\nDescargando el fichero del INE 2018:\nutils::download.file(url = \u0026quot;http://www.ine.es/pcaxisdl/t20/e245/p05/a2018/l0/00000006.px\u0026quot;,\rdestfile = \u0026quot;data/census_2018.px\u0026quot;)\r\ntbl_census_2018 \u0026lt;- read.px(\u0026quot;data/census_2018.px\u0026quot;) %\u0026gt;% # Load \u0026amp; format\ras_tibble()\rParseamos los datos con el objetivo de conseguir un dataframe que consista en name, postal_code, average_age\ntbl_census_2018 %\u0026lt;\u0026gt;% set_names(c(\u0026quot;age\u0026quot;, \u0026quot;city\u0026quot;, \u0026quot;sex\u0026quot;, \u0026quot;population\u0026quot;)) %\u0026gt;% # Cambiamos los nombre\rna.omit() %\u0026gt;% # Na rmv\rfilter((city!=\u0026quot;Total\u0026quot;)\u0026amp;(age!=\u0026quot;Total\u0026quot;)\u0026amp;(sex==\u0026quot;Ambos sexos\u0026quot;)) %\u0026gt;% # Duplicate info rmv\rseparate(city, c(\u0026#39;postal_code\u0026#39;, \u0026#39;city_name\u0026#39;), sep=\u0026quot;-\u0026quot;) %\u0026gt;% # Sep City column\rmutate(age = as.numeric.factor(age)) %\u0026gt;% # Conv to numeric\rgroup_by(city_name, postal_code) %\u0026gt;% # Group to operate\rsummarise(avg_age = sum(population*age,na.rm = T)/sum(population,na.rm=T)) %\u0026gt;% # Avg age\rselect(city_name, postal_code, avg_age) # Discard columns\rkable( # Just Rmarkdown format\rtbl_census_2018 %\u0026gt;% head(2)\r)\r\r\rcity_name\rpostal_code\ravg_age\r\r\r\rAbabuj\r44001\r52.40789\r\rAbades\r40001\r45.40000\r\r\r\r\n\r2.2- Descargando datos GIS\rLa segunda fuente de datos que vamos a utilizar son los datos geográficos de los municipios españoles, los cuales cruzaremos con los censales anteriormente descargados para pintar la edad media en sus respectivas coordenadas.\nDescargando los daots GIS:\ntemp \u0026lt;- tempfile() # Create the tempfile\ru=\u0026quot;http://www.arcgis.com/sharing/rest/content/items/8e31c4c1a0b348f79058f212d0d807a1/data\u0026quot;\rutils::download.file(url = u, destfile = temp,\rmode=\u0026quot;wb\u0026quot;) # Binary mode for correct download\runzip(temp, exdir = \u0026quot;data/cities_gis\u0026quot;) # Unzip in data/cities_gis\runlink(temp) # Delete temp file\rTratamos estos datos para convertirlos de formato espacial a datos tabulares. Para este caso concreto de 3D, las Islas Canarias podrían deformarnos el gráfico, por lo que decidimos permanecer concentrados en nuestro objetivo didáctico y filtramos estas coordenadas. Por supuesto es posible mantenerlas sin perjudicar el gráfico, alterando sus coordenadas y acercándolas a la península. ¡Esto te queda como tarea para ti!\nPara llevar a cabo este procesado de los datos, usamos la función fortify para no depender de más paquetes. No obstante esta funcion nos lanza un warning sugiriendonos el uso de la función tidy() del paquete broom.\ntlb_cities_gis \u0026lt;- readOGR(dsn = \u0026quot;data/cities_gis/Municipios_ETRS89_30N.shp\u0026quot;,\rverbose=FALSE) # Spatial data reading\rtlb_cities_gis %\u0026lt;\u0026gt;% fortify(region = \u0026quot;Codigo\u0026quot;) # %\u0026gt;% # Conv \u0026quot;spatial object\u0026quot; to data.frame\r# broom::tidy()\rplot_canarias \u0026lt;- F # Control param, initial app config\rif(plot_canarias==F){ # Should be moduled in a funct\rtlb_cities_gis %\u0026lt;\u0026gt;%\rfilter((long\u0026gt;0) \u0026amp; (lat\u0026gt;4000000)) # Filter peninsular data\r} \rPara terminar, joineamos los dos datasets creados para conformar el tablón final, el cual vamos a usar como base para crear las gráficas. Apuntar que usamos left join como forma de mantener los datos geos y no perder coordenadas en el plot.\ntbl_cities_avg_age \u0026lt;- tlb_cities_gis %\u0026gt;% left_join(tbl_census_2018, by = c(\u0026quot;id\u0026quot; = \u0026quot;postal_code\u0026quot;)) \rComo buena práctica, comprobamos el número de NAs generados a partir de este left join. Estos NAs serán municipios de los que tenemos coordenadas pero no contamos con información sobre la edad media.\nVemos que los valores perdidos representan únicamente el 1% del total de filas, por lo que vamos a imputarlos con el dato del código postal previo. Es cierto que podemos mejorar esta imputación, pero para nuestro propósito será suficiente debido al pequeño porcetaje del total que representan. ¡Vuelve a quedar de tu mano mejorarlo y comentárnoslo!\nkable( # RMarkdown output format\rtbl_cities_avg_age %\u0026gt;%\rgroup_by(id) %\u0026gt;%\rsummarise(na = sum(is.na(avg_age))) %\u0026gt;% # NAs by city\rsummarise(missing_perc = sum(na\u0026gt;0)/length(na)*100) %\u0026gt;% # Perc cities with at least 1 na select(missing_perc)\r,\ralign = \u0026quot;c\u0026quot;\r)\r\r\rmissing_perc\r\r\r\r0.9268413\r\r\r\rtbl_cities_avg_age %\u0026lt;\u0026gt;% arrange(id) %\u0026gt;% fill(avg_age, .direction = \u0026quot;down\u0026quot;) # Fill with the previous pc data.\r\n\r2.3- Visualización con Ggplot\rInspirado en gran medida en http://blog.manugarri.com/making-a-beautiful-map-of-spain-in-ggplot2/\nCon este dataset final, plotearemos las variables que representan las coordenadas en el eje X e Y y en primer lugar representaremos la edad media mediante la paleta de color. Las tonalidades rojas son asignadas a edades superiores y las azules a las edades medias más jóvenes. Conseguimos esto mediante el aesthetic fill de Ggplot.\nmyPalette \u0026lt;- colorRampPalette(rev(brewer.pal(11, \u0026quot;Spectral\u0026quot;))) # Create reverse Spectral palette\rplot_cities \u0026lt;- ggplot() +\rgeom_polygon(data = tbl_cities_avg_age, aes(fill = avg_age, x = long, y = lat, group = id)) + # Dummy variable to correct fill by PCode.\rscale_fill_gradientn(colours=myPalette(4)) + # Choose palette colours.\rlabs(fill=\u0026quot;Avg age\u0026quot;)\rplot(plot_cities)\r\n\r2.4- Visualización en 3D con Rayshader!\rEl anterior gráfico estaba bastante bien. En el podemos facilmente distinguir los municipios con la edad media más alta y los municipios más jóvenes. Sin embargo, los ojos humanos no son capaces de distinguir fácilmente entre colores próximos ni distinguir la magnitud de las diferencias en esta escala. Por lo tanto, ¿qué tal complementarlo con un nuevo eje?\nVeamos como hacerlo y que tal queda\nplot_gg(plot_cities,multicore=TRUE,width=5,height=3,scale=310) # Plot_gg de rayshader\rrender_snapshot()\r\nHmm dijiste algo sobre render_movie()… Qué tal si lo animamos?\n\n\r2.5- Animación 3D con Rayshader\rEn el gráfico anterior, la variable edad media queda bastante más entendible por el ojo humano en la dimensión añadida. Aquí la elección del ángulo e inclinación correctos es un punto esencial. Pero, ¿y mejoramos la interpretabilidad rotando el gráfico?\nEsto es de lo que se encarga la siguiente función:\nrender_movie(\u0026quot;img/movie_spain.mp4\u0026quot;,frames = 720, fps=30,zoom=0.6,fov = 30)\r\rbody {\rtext-align: justify}\rp {\rword-spacing: 3px;\r}\r\r\r\r","date":1564185600,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":1569354975,"objectID":"bfeedc1134e9c3fbf93df2c48c1431e8","permalink":"https://typethepipe.com/es/post/de-ggplot-a-3d-en-r-con-rayshader/","publishdate":"2019-07-27T00:00:00Z","relpermalink":"/es/post/de-ggplot-a-3d-en-r-con-rayshader/","section":"post","summary":"En 7 minutos, seremos capaces de convertir nuestras gráficas generadas con ggplot en espectaculares plots en 3D, ¡y ademas interactivos!\rPodrás embebernos en HTML/Rmarkdown, o incluso mejor, podras exportarlo como mp4 en una animacion rotatoria para sacarle todo el jugo a tus datos!\nComo caso de uso, vamos a visualizar la edad media de los municipios españoles cruzando datos del padrón con los datos GIS, para posteriormente visualizarlos en 3 dimensiones.","tags":["rayshader","ggplot","viz","vizR","visualization"],"title":"Convierte tu GGplot en una animación 3D con R y Rayshader","type":"post"},{"authors":["Carlos Vecina"],"categories":["Tips","Datathon"],"content":"\rKaggle - https://www.kaggle.com\nAgorize - https://www.agorize.com/en/challenges\nanalyticsvidhya - https://datahack.analyticsvidhya.com/contest/all\nhackerearth - https://www.hackerearth.com/challenges/competitive/\nA nivel nacional y local están comenzando a organizarse todo tipos de eventos relativos al #opendata. Te invitamos a que investigues sobre ellos y te animes a colaborar en mejorar tu entorno a través de la explotación de los datos.\nPor ejemplo, en España a fecha de este artículo, los más importantes son:\rCajaMar - https://www.cajamardatalab.com/\nGobierno - https://datos.gob.es/es/event-tags/concurso\nJunta Castilla y Leon - https://datosabiertos.jcyl.es/web/es/datos-abiertos-castilla-leon.html\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"es","lastmod":-62135596800,"objectID":"b5d2a0df261c0114223712a65f18d657","permalink":"https://typethepipe.com/es/vizs-and-tips/plataformas-donde-participar-competiciones-de-datos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/es/vizs-and-tips/plataformas-donde-participar-competiciones-de-datos/","section":"vizs-and-tips","summary":"No todo es Kaggle","tags":[],"title":"Plataformas donde puedes participar en competiciones de datos","type":"vizs-and-tips"}]