---
title: "10 consejos para triunfar en tu próximo Datathon"
author: Carlos Vecina Tebar
date: "2020-02-17"
slug: "10-consejos-datathon"
translationKey: "10-advices-datathon"
output:
  blogdown::html_page:
    highlight: tango
categories:
- Post
- Datathon
tags: []
subtitle: ""
summary:  "¿Quieres conseguir tus objetivos?"
authors: [carlos-vecina]
featured: false
draft: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



En este post os traemos 10 consejos sobre los puntos clave para conseguir grandes resultados en un Datathon o competición de datos. 

<br>

**1. Establece tu objetivo y revisa los recursos(habilidades, tiempo, acceso a máquinas...) con los que cuentas.**

Comenzamos por un punto esencial. Dependiendo de tu perfil y de la competición, deberás plantearte a priori qué pretendes conseguir participando en el reto. Puede ser aprender y mejorar en un determinado campo o tecnología, conseguir el premio, quedar entre los 3 primeros para incluirlo en tu CV o simplemente por diversión. Cualquiera que sea está bien. 

Lo que recomendamos es evaluarlo de manera sincera, ya que será bastante el tiempo invertido y tener tu objetivo presente te ayudará en los momentos de menor motivación.

<br>


**2. Elección del tema/industria/CdUso.**

Ligado con esta última idea, dado que vamos a dedicar esfuerzos en este proyecto, ¿cómo no elegir un tema, campo o caso de uso que nos motive? Afortunadamente, desde muchas organizaciones y plataformas se están lanzando diferentes e interesantes competiciones. Kaggle es uno de lso máximos exponentes de estas plataformas, pero también existen otra, de las que hablamos [aquí](../../vizs-and-tips/plataformas-donde-participar-competiciones-de-datos).

<br>

**3. Construye bien tu equipo. Usa herramientas para compartir codigo/archivos.**

Dependiendo de los dos puntos anteriores, especialmente del primero, dependiendo de tu objetivo deberás elegir a tu equipo de manera consecuente. Elegir a un compañero o amigo con el que te lleves bien y os motiveis mutuamente si tu objetivo es aprender sobre un tema o tecnología nueva. Elige a alguien que comparta tus intereses en el caso que desees dedicarle tiempo a profundizar un un tema, tecnología o algoritmo muy concreto. Si el proyecto lo requiere, y pretendes optar al premio o a posiciones altas, te recomendamos elegir un compañero que te complemente. En el caso de que no se requiera multidisciplinariedad, opta por aquellos que tenga un nivel similar al tuyo.  Si es posible, ligeramente superior.

<br>

**4. Investigación profunda sobre el caso en concreto.**

Una vez formado el equipo, os recomendamos comenzar a investigar el negocio o el contexto en la que se enmarca vuestro proyecto. Esto permitirá plantear mejor el trabajo, extraer un valor muy superior a los datos y os evitará iteraciones en el proceso diseñar-implementar-evaluar al partir de premisas más claras, dejando al lado una mala interpretación de conceptos básicos, lo cual puede ser letal.

Un ejemplo. Si la competición trata sobre predecir el número de visitas o el número de contrataciones de determinados productos en un comercio online, debemos controlar el comportamiento de los datos que recoge Google Analytics. Conocer el significado de tasa de rebote, las mecánicas desde que un usuario se conecta y se le asigna una cookie hasta que convierte, diferentes casuísticas de registros nulos, bots, que la *source*(para conocer la fuente de donde viene el visitante) en caso de dudas se asigna como *direct*...

Sin esta serie de conocimientos sería dificil *craftear* los datos para conseguir un buen *performance* de los modelos, pero más importate, cualquier resultado que obtengas será malinterpretado. Por lo tanto, todo este *background* no es imprescindible para llegar a algún resultado, pero sí lo será para llegar a resultados potentes y realizar una participación de la que acabeis orgullosos.

<br>

**5. Análisis exploratorio de los datos. Recuerda que (usualmente) no son los únicos que puedes usar.**

Como todo proyecto de datos, consta de una serie de etapas iterativas. Después de investigar sobre el contexto, echarás un primer vistazo a los datos. Si surgen dudas, de nuevo habrá que investigar sobre ellas. 

Al margen de este recordatorio, la fase exploratoria se centra en conocer cada una de las variables. Como consejo personal, entre otras cosas, nosotros comenzamos midiendo el porcentaje de NAs y la varianza de las variables. Aquellas que tengan un elevado porcentaje de NAs o una varianza muy pequeña, o bien las apartamos (podría replantearse su entrada en siguientes iteraciones) o bien las *encodeamos* de manera dicotómica Si_NA/No_NA o Mayoritaria/No_mayoritaria. Para otro tipo de encodings ver XXXXXX

Este paso varía bastante dependiendo del número de variables que contenga tu dataset. En *datasets* grandes de 75, 100 o más variables, resultará más complicado hacerse una idea general de las características de cada una de ellas. Esto puede resultar ciertamente abrumador, sin embargo, tras una buen estudio, eleccion y transformación, puede ser que acabes dando las gracias de no tener sólo 5 o 6 variables(caso en el que se suele llegar a un *plateau* de desempeño de los modelos más rápido y mayor homogeneidad en los resultados expuestos por los equipos).

Por último, recordar que normalmente no hay problema con utilizar datos complementarios a los propios de la competición. En algunos casos como Kaggle, lo que se establece es la obligación de comentar y hacer público el uso de estos datos durante la competición. 

Datos demográficos que ayuden a poner en contexto las variables geográficas como código postal o provincia, aportar explicabilidad a *spikes* o eventos pasados... son sólo algunos de los ejemplos de datos que se pueden incorporar al dataset original del reto, obviamente teniendo en cuenta cuales de estos datos vamos a tener y cuales no a la hora de predecir.

<br>

**6. Establecer la estructura del código.**

A continuación os mostramos la estructura que suelen tener nuestros proyectos. Llevamos a cabo una validación cruzada manual con el objetivo de ganar flexibilidad a la hora de usar modelos de diferente naturaleza y poder compararlos y combinarlos. Cierto es que soluciones paquetizadas como scikit-learn o H2O pueden hacer este trabajo en el caso de que el tiempo disponible y características de la competición indiquen su uso.

Carga de entorno (paquetes y funciones)
Diferentes craft de variables
Seperación en dataset y datasetOOSample
Separar el dataset en *folds*
Por cada fold:
  Entrenar con el resto
  Predecir en el fold
  Evaluación
Predicción OOSample
Evaluación
Predicción del conjunto de test a enviar.

<br>


**7. Lanzar los primeros modelos.**

En primer lugar centrate en la familia de algoritmos que a priori mejor se adapten a tus datos, al objetivo a predecir, incluso a la métrica de error por la que se te va a medir. Es importante sacar los primeros resultados, a ser posible dentro de una estructura similar a la propuesta anteriormente. Las primeras métricas de error te ayudaran en muchos aspectos. Por un lado te pueden dar pistas de que *bugs* en el código(metricas de error irrealistas). También te servirá de base la cual ir mejorando, y como estamos seguros de que lo harás, esto será ademañs un *boost* de ánimo.

Como principales familias XXXXXX

<br>

**8. Validación cruzada, OOS y backtesting.**

Especial mención a las métricas de error. Una de las características a la que de manera personal más importancia le damos, se tratata de conocer los intervalos de error de nuestros modelos antes de enviarlos. Conocer las precisión de nuestro modelo bajo diferentes escenarios, partes del dataset e incluso datos sintéticos nos hace sentir un especial orgullo. Quizá sea simplemente un objetivo que nos marcamos(tal y como comentamos en el punto 1), pero el hecho de tener la certeza de cuando bajaos la métrica de error no se debe a algo espúreo, o lo peor de todo a un *bug*, nos permite desarrollar con tranquilidad. Además en el caso de existir un ranking público, controlar tu métrica de evaluación te permite saber con mayor certeza en que percentil te encuentras.

Estas competiciones suelen ir ligadas a una métrica de error por la cual se evalua a cada participante. Será fundamental introducirla en nuestra evaluación de modelos, pero no debe de ser la única. Tanto a la hora de fijar una función objetivo como a la hora de evaluar los modelos, debemos tener en cuenta métricas de error complementarias que nos ayuden a interpretar el desempeño de los modelos. Tener en cuenta el error mediano o el MAPE, cuando la métrica con la que nos evaluarán será el error medio, nos puede ayudar a detectar elegir entre modelos que difieren en sólo un x% en esta métrica pero no en otras, en un determinado conjunto de datos. 

<br>

**9. Interpretación y evaluación de resultados desde el punto de vista práctico y/o negocio.**

En el caso de que se trate de un Datathon organizado por alguna empresa de tu zona, seguramente los mejores proyectos serán llamados a un evento final de presentación de los trabajos. En otros, te pedirán una memoria presentando los resultados. En el caso de Kaggle, no suelen hacerlo.

Por lo tanto, si la competición a la que te presentas requiere de este tipo de resultado, será importante tenerlo presente no sólo al final, sino durante todo el proceso. Una vez que hemos re-optimizado los hiperparámetros con lo última librería bayesiana, que hemos *stackeado* nuestros mejores modelos, quizá sea hora de trabajar en la interpretabilidad de los resultados, incorporación de nuevos datos, en definitiva, conclusiones que sean valiosas a la hora de poner en marcha el modelo y que no quede en una simple matriz de pesos.

En la última competición a la que asistimos donde se nos pedía esta presentación final a los tres mejores proyectos, e incluso aunque no se hubiese pedido, optamos por adentrarnos en el modelado de las imagenes mediante *transfer learning*, cosa que no se nos pedía. Una vez que modelamos la información estructurada y llevamos a cabo ciertas iteraciones sobre estos datos, pensamos que sería más enriquecedor tanto para nosotros como para las personas que proponían el concurso, la investigación sobre la extracción de información de las imagenes (información que en el caso de uso que nos ocupaba, a priori, debería tener un impacto claro sobre la variable a predecir).

<br>

**10. (Personal) Escribir una memoria con puntos fuertes y puntos a mejorar detectados, que te sirvan de partida en el próximo reto.**

Una vez finalizado el proyecto, un práctica que nos ha enriquecido y ha supuesto un antes y un después ha sido escribir algún tipo de memoria o documento, reflexionando sobre la reciente participación. Analizando los puntos fuertes, lo que debimos mejorar, tanto a nivel técnico como a nivel de equipo y por qué no a nivel emocional y de actutid. A ser posible nada más entregar y antes de saber el resultado. Sería genial si al tiempo de conocer el mismo se añadiera algún punto o reflexión más.

Todas estas anotaciones nos ayudan cada vez que nos enfrentamos a un proyecto a encararlo más preparados y no depender del 'instinto' ni de lo que la memoria selectiva nos quiera recordar en momentos puntuales. 

Una mala predisposición personal, una mala elección de equipo, no estudiar suficientemente y con cariño los valores extremos, no preparar la presentación con tiempo, correr los modelos 30 minutos antes de que acabe el plazo de entrega... son situaciones que nos han pasado a muchos y no ocurre absolutamente nada por reflexionar sore ellas y dejarlo por escrito. Lo fundamental es, dentro de lo posible, ¡que no se repitan en la próxima competición! :)







<style>
body {
text-align: justify}
p {
  word-spacing: 3px;
}
</style>

<script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script><script type="text/javascript">window.dojoRequire(["mojo/signup-forms/Loader"], function(L) { L.start({"baseUrl":"mc.us4.list-manage.com","uuid":"91551f7ed29389a0de4f47665","lid":"d95c503a48","uniqueMethods":true}) })</script>
