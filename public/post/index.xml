<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | TypeThePipe</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 21 Sep 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Energy forecasting error metrics</title>
      <link>/post/energy-forecasting-error-metrics/</link>
      <pubDate>Sat, 21 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/post/energy-forecasting-error-metrics/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scale-dependent-error-metrics&#34;&gt;Scale dependent error metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#percentage-error-metrics&#34;&gt;Percentage-error metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scale-free-error-metrics&#34;&gt;Scale-free error metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The idea of this post comes from the different error metrics I have dealt with working with time series data and forecasting models.&lt;/p&gt;
&lt;p&gt;Among other things, we make energy production forecasts of renewable power plants of different capacities and technologies.
Our aim is to develop forecasting models that reduce the penalties caused by the deviations.&lt;/p&gt;
&lt;p&gt;Most of the models I work with are regression models, and therefore in this article I am focusing only on regression error metrics.&lt;/p&gt;
&lt;p&gt;Unfortunately, there is no absolute “right” accuracy metric.
Choosing the right metric is a problem-specific matter, and it involves answering questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Which decision will you base on the forecast?&lt;/li&gt;
&lt;li&gt;What are the consequences of a wrong forecast?&lt;/li&gt;
&lt;li&gt;Who is going to check and monitorize the errors?&lt;/li&gt;
&lt;li&gt;Do we care about the percentage error or about the magnitude of the deviation?&lt;/li&gt;
&lt;li&gt;Does it makes any difference to over-forecast or under-forecast the variable of interest?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Answering the above questions lead us to determine we need to find a metric that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is scale independent, so the errors are comparable between power plants.&lt;/li&gt;
&lt;li&gt;Is symmetric, as we do not want to weight the deviations differently according to their sign.&lt;/li&gt;
&lt;li&gt;Express the error in absolute terms, so the error reflects the real-life imbalance costs.&lt;/li&gt;
&lt;li&gt;The error calculated over different periods should be equivalent to the aggregated calculation over those periods individually.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each metric behaves in a certain way and therefore reflects in a unique manner the features of the models.
Depending on their properties, we can classify the metrics in several categories.
Let’s take a look at them:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scale-dependent-error-metrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Scale dependent error metrics&lt;/h1&gt;
&lt;p&gt;Maybe the most popular and simple error metric is &lt;strong&gt;MAE&lt;/strong&gt;:&lt;/p&gt;
&lt;div id=&#34;mae&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;MAE:&lt;/h3&gt;
&lt;p&gt;The Mean Absolute Error is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ MAE = \frac{1}{N}\sum_{t=1}^{N} |A_t - F_t| \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While the MAE is easily interpretable (each residual contributes proportionally to the total amount of error), one could argue that using the sum of the residuals is not the best choice, as we could want to highlight especially whether the model incur in some large errors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mse-rmse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;MSE &amp;amp; RMSE&lt;/h3&gt;
&lt;p&gt;For those cases, maybe &lt;strong&gt;MSE&lt;/strong&gt; (Mean Squared Error) or &lt;strong&gt;RMSE&lt;/strong&gt; (Root Mean Squared Error) are a better choice. Here the error grows quadratically and therefore extreme values penalize the metric to a greater extent.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ MSE = \frac{1}{N} \sum_{t=1}^{N} |A_t - F_t|^2  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ RMSE = \sqrt{MSE}  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The main problem with scale dependent metrics is that they are not suitable to compare errors from different sources.&lt;/p&gt;
&lt;p&gt;In our case, the capacity of the power plants would determine the magnitude of the errors and therefore comparing them between facilities would not make much sense.
This is something we should try to avoid when choosing the metric.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;percentage-error-metrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Percentage-error metrics&lt;/h1&gt;
&lt;p&gt;Next group express the error in percentage terms.&lt;/p&gt;
&lt;div id=&#34;mape&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;MAPE&lt;/h3&gt;
&lt;p&gt;The most widespread one is the Mean Absolute Percentage Error:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ MAPE_\% = \frac{1}{N}\sum_{t=1}^{N} \frac{|A_t - F_t|}{A_t} \times 100\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As we said above, depending on our goal, &lt;strong&gt;MAPE&lt;/strong&gt; could be suitable or not. From my point of view percentage error metrics have some major drawbacks.
They may give different values for two observations with the same absolute error, depending on whether they share the same actual value or not:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Case
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Actual
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Forecast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
AbsoluteDifference
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
MAPE
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
150
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
150
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Besides, MAPE diverges when actual values tend to zero.
In our case it is impractical, as this could lead to extreme cases such as:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Case
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Actual
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Forecast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
AbsoluteDifference
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
MAPE
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1000
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;That is an undesired behaviour for an error metric since we don’t want to assign huge errors to deviations that involve insignificant operating costs. That suggests a first strong conclusion:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We need to find error metrics that are aligned with our business goals.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Besides, in the example above we can see that MAPE isn’t symmetric as it weights differently two residuals whether the forecast is above or below the actual value.
That idea of symmetry lead us to sMAPE.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;smape&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;sMAPE&lt;/h3&gt;
&lt;p&gt;Trying to solve that assymetry, an alternative to MAPE was proposed. It is called &lt;strong&gt;sMAPE&lt;/strong&gt;, which stands for Symmetric Mean Absolute Percentage Error:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ sMAPE_\% = \frac{2}{N} \sum_{t=1}^{N}\frac{|A_t - F_t|}{|A_t| + |F_t|} \times 100\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, against all odds &lt;a href=&#34;../Symmetric-MAPE-is-not-symmetric&#34;&gt;Symmetric MAPE is not symmetric&lt;/a&gt;: as MAPE, it may present different values for the same absolute deviation:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Case
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Actual
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Forecast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
AbsoluteDifference
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sMAPE
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
150
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For our use case it is very inconvenient that the same absolute deviation may be quantified with two different error values.&lt;/p&gt;
&lt;p&gt;This is a key question: We don’t want to minimize the percentage error but to minimize the economic losses due to forecast deviations, and they are exclusively connected to the sum of the absolute errors.
Therefore, we should evaluate the accuracy based on that criteria.&lt;/p&gt;
&lt;p&gt;As a final point, simply mention that some others have proposed the &lt;strong&gt;Log Ratio&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(ln(F_t/A_t)\)&lt;/span&gt; as a better alternative to MAPE.
You can read a brief description in the &lt;a href=&#34;../Symmetric-MAPE-is-not-symmetric&#34;&gt;previously mentioned sMAPE article&lt;/a&gt; or
an extended discussion in the &lt;a href=&#34;https://poseidon01.ssrn.com/delivery.php?ID=912071094074109094069115029093025110000069085067010018086003069122074000022002100121034011107062116011014121092068067002120016015069090069028013066084099122096112019086019005105020067099126068064011082116019102096003091124077118086097090097087121114&amp;amp;EXT=pdf&#34;&gt;original paper&lt;/a&gt; by Chris Tofallis&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;scale-free-error-metrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Scale-free error metrics&lt;/h1&gt;
&lt;p&gt;These are error metrics that have been conveniently normalized to make them dimensionless.&lt;/p&gt;
&lt;p&gt;The main advantages of these metrics are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Same absolute deviations lead to the same error.&lt;/li&gt;
&lt;li&gt;They are symmetric.&lt;/li&gt;
&lt;li&gt;They are comparable between power plants.&lt;/li&gt;
&lt;li&gt;They are connected to our economic goals.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;nmae&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;NMAE&lt;/h3&gt;
&lt;p&gt;First of all we have &lt;strong&gt;NMAE&lt;/strong&gt; that stands for Normalized Mean Absolute Error.
This metric is specific to the energy forecasting business as it is normalized by the capacity &lt;em&gt;C&lt;/em&gt; of the power plant, but one could generalize it to any other area provided that there exist an upper bound for the forecasts.&lt;/p&gt;
&lt;p&gt;NMAE is expressed as a percentage. It is our preferred metric as it is truly connected with the business goals, it’s easily interpretable and comparable between plants.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ NMAE_\% = \frac{1}{N}\sum_{t=1}^{N} \frac{|A_t - F_t|}{C} \times 100\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Besides, it shows the desirable property:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ NMAE_{p1 ∪ p2}  = \frac{1}{2}(NMAE_{p1}  +   NMAE_{p2})\]&lt;/span&gt;
Given both periods have the same length. If not (e.g: consecutive months), you would only have to adjust by their relative length.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The real-life cost of a forecast error is proportional to the absolute value of the residuals.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The only case when this metric does not apply is whenever the &lt;em&gt;capacity&lt;/em&gt; notion has no sense: If the range of the possible values is not bounded, what normalizing constant should I choose?&lt;/p&gt;
&lt;p&gt;This would be the case when forecasting temperatures or electricity market prices.
Using MAE could be appropiate in these cases, as the units are in the same scale than the magnitude (ºC or €/MWh) and so the errors are easily interpretable, although they would not be truly comparable across different markets or locations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;madsum-ratio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;MAD/Sum Ratio&lt;/h3&gt;
&lt;p&gt;In energy-related businesses, I have spotted another error metric usually (and, as far as I know, wrongly) called &lt;strong&gt;WMAE&lt;/strong&gt;.
Now, WMAE should stand for “Weighted Mean Average Error”. However, the definition I stumbled upon several times was:
&lt;span class=&#34;math display&#34;&gt;\[ \frac{1}{N}\frac{ \sum_{t=1}^{N}{|A_t - F_t|}}{\sum_{t=1}^{N}{A_t}} \times 100\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is basically the MAE normalized by acummulated energy production.&lt;/p&gt;
&lt;p&gt;It resembles &lt;strong&gt;MAD/Mean Ratio&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ MAD/Mean Ratio_\% = \frac{1}{N}\frac{ \sum_{t=1}^{N}{|A_t - F_t|}}{\frac{1}{N}\sum_{t=1}^{N}{A_t}} \times 100\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From my point of view, and as an analogy of the MAD/Mean Ratio, the first expression should be called &lt;strong&gt;MAD/Sum Ratio&lt;/strong&gt;.
Their properties are similar:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Their range is [0, &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;) for non-negative values, which may be difficult to interpret.&lt;/li&gt;
&lt;li&gt;They both show the same asymmetry as MAPE: Different error values come from the same absolute difference between forecasts and actuals.&lt;/li&gt;
&lt;li&gt;Small absolute deviations may be associated to big MAD/Mean or MAD/Sum Ratios, given the actual values are small.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For all those reasons, we insist on the idea that they are kind of &lt;em&gt;disconnected&lt;/em&gt; from our loss function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mase&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;MASE&lt;/h3&gt;
&lt;p&gt;There are other scale-free metrics. One of them is &lt;strong&gt;MASE&lt;/strong&gt; (Mean Absolute Scaled Error), proposed by Rob J. Hyndman:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ MASE =\frac{\frac{1}{J} \sum_j |A_j - F_j|} {\frac{1}{T-1}\sum_{t=2}^T |A_{t}-A_{t-1}|}  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the numerator is the error in the forecast period and the denominator is the MAE of the one-step “naive forecast method” on the training set, that is &lt;span class=&#34;math inline&#34;&gt;\(F_t = A_{t-1}\)&lt;/span&gt;.
Because of that, MASE is a metric specifically designed for time series.&lt;/p&gt;
&lt;p&gt;Again, whether it is suitable for your needs or not depends entirely on the problem.
While it has certain interesting properties such as scale-independence, convergence when &lt;span class=&#34;math inline&#34;&gt;\(A_t\to0\)&lt;/span&gt; and symmetry,
in our case this metric isn’t optimal for several reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The training series must be completed, i.e., with no gaps. In our case, sometimes we have some measurements missing.&lt;/li&gt;
&lt;li&gt;MASE is equal to 1 when the forecast performance is similar to the naive forecast in the training set. That implies a dependence with the historic period that isn’t always very convenient: if in any given moment we happen to recieve some missing historical production measurements, the accuracy of our model suddenly would change, which may be kind of unintuitive and difficult to track through time.&lt;/li&gt;
&lt;li&gt;MASE is unbounded on the upper side.&lt;/li&gt;
&lt;li&gt;It doesn’t seem to be a friendly metric to use with non-technical people, such as clients or stakeholders: How large are the expected losses for a 1.2 MASE?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;emae&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;EMAE&lt;/h3&gt;
&lt;p&gt;I found yet another scale-free error metric in a &lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=3&amp;amp;ved=2ahUKEwjA3rihoO7jAhWLzoUKHeKJCZUQFjACegQIABAC&amp;amp;url=https%3A%2F%2Fpdfs.semanticscholar.org%2Fcf04%2F65bce25d78ccda6d8c5d12e141099aa606f4.pdf&amp;amp;usg=AOvVaw0FlIvYKTyxgOYX7xCo0PAz&#34;&gt;recent paper&lt;/a&gt; from the Energy Department of the Politecnico di Milano,&lt;/p&gt;
&lt;p&gt;They called it &lt;strong&gt;EMAE&lt;/strong&gt; (Envelope-weighted Mean Absolute Error):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ EMAE_\% = \frac{1}{N}\frac{ \sum_{t=1}^{N}{|A_t - F_t|}}{\sum_{t=1}^{N}{\max(A_t, F_t)}} \times 100\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This metric is quite similar to the MAD/Sum Ratio above but divides by the sum of the maximum between the forecast and the measured power for each observation. It is also expressed as a percentage. This function shows some nice properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is scale-independent.&lt;/li&gt;
&lt;li&gt;It is symmetric.&lt;/li&gt;
&lt;li&gt;It maps absolute deviation to one unique value.&lt;/li&gt;
&lt;li&gt;It is easily interpretable as its range is [0,100].&lt;/li&gt;
&lt;li&gt;It doesn’t diverge in any point.&lt;/li&gt;
&lt;li&gt;It’s a nice alternative to NMAE since a &lt;em&gt;capacity&lt;/em&gt; value is not required.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This formula allows for a cool graphic interpretation of the error: the numerator matchs with the yellow area whereas the denominator corresponds to the sum of the blue and yellow areas:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Forecasting-error-metrics_files/EMAE_plot.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;There is no “best metric” to measure model performance. There are several metrics that highlight different characteristics.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;One key aspect is to find error metrics that are connected with our objectives.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Since in most cases the real-life cost of a forecast error is proportional to the absolute value of the residuals, the choice of the metric should be consistent with it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For our case, NMAE presents the ideal characteristics of interpretability, stability and relation with our loss function that make it the optimal choice.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;EMAE is proposed as a nice alternative for the cases NMAE cannot be applied.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://robjhyndman.com/papers/foresight.pdf&#34;&gt;Another look at forecast-accuracy metrics for intermittent demand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://otexts.com/fpp2/accuracy.html&#34;&gt;Forecasting: Principles and Practice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://robjhyndman.com/hyndsight/smape&#34;&gt;Errors on percentage errors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0174202&#34;&gt;A new accuracy measure based on bounded relative error for time series forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/publication/322902867_Comparison_of_Training_Approaches_for_Photovoltaic_Forecasts_by_Means_of_Machine_Learning/link/5a7512b1aca2722e4ded13ad/download&#34;&gt;Comparison of Training Approaches for Photovoltaic Forecasts by Means of Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0169207099000072&#34;&gt;On the assymetry of the symmetric MAPE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://poseidon01.ssrn.com/delivery.php?ID=912071094074109094069115029093025110000069085067010018086003069122074000022002100121034011107062116011014121092068067002120016015069090069028013066084099122096112019086019005105020067099126068064011082116019102096003091124077118086097090097087121114&amp;amp;EXT=pdf&#34;&gt;A better measure of relative prediction accuracty for model selection and model estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.forecastpro.com/Trends/forecasting101August2011.html&#34;&gt;A Guide to Forecast Error Measurement Statistics and How to Use Them&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Symmetric MAPE is not symmetric</title>
      <link>/post/symmetric-mape-is-not-symmetric/</link>
      <pubDate>Mon, 26 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/symmetric-mape-is-not-symmetric/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I recently came across &lt;a href=&#34;https://robjhyndman.com/hyndsight/smape&#34;&gt;this interesting discussion&lt;/a&gt; about percentage errors and I would like to talk about MAPE and its characteristics.&lt;/p&gt;
&lt;p&gt;MAPE is the acronym of Mean Absolute Percentage Error and is defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[MAPE = \frac{100}{N} \sum_{t=1}^{N} \frac{|A_t - F_t|}{|A_t|}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;being &lt;span class=&#34;math inline&#34;&gt;\(A_t\)&lt;/span&gt; the &lt;em&gt;Actuals&lt;/em&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F_t\)&lt;/span&gt; the &lt;em&gt;Forecasts&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There is some confusion and disagreement about the behaviour of MAPE.&lt;/p&gt;
&lt;p&gt;MAPE express the deviation in relative terms and provides a simple interpretation of the error.
It is easy to calculate and communicate and probably that is why it has been widely used in forecasting business.&lt;/p&gt;
&lt;p&gt;However, it suffers from some known issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-problems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The problems&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;MAPE ranges from 0 to &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;: it diverges with &lt;span class=&#34;math inline&#34;&gt;\(A_t \to 0\)&lt;/span&gt; which leads to problems when dealing with data with zero values such as intermittent demand data or energy forecasting.
This problem is even worse when working with data with arbitrary zero values, e.g, forecasting temperatures near 0&lt;span class=&#34;math inline&#34;&gt;\(^o\)&lt;/span&gt; in Celsius or Fahrenheit scale. To be fair, this would be an issue using any percentage error metric.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;MAPE &lt;strong&gt;is said to be asymmetric&lt;/strong&gt; in the sense it puts heavier penalty on negative errors. When &lt;span class=&#34;math inline&#34;&gt;\(F_t &amp;lt; A_t\)&lt;/span&gt; the maximum possible error value is 100%,
however there is no limit when forecasting on the high side. Besides, given the same absolute difference &lt;span class=&#34;math inline&#34;&gt;\(|A_t - F_t|\)&lt;/span&gt;, MAPE is greater when &lt;span class=&#34;math inline&#34;&gt;\(F_t &amp;gt; A_t\)&lt;/span&gt;:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Case
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Actual
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Forecast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
AbsoluteDifference
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
MAPE
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
150
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
150
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;However, there is some controversy over this last point.
Some people argue this is a false dichotomy because it doesn’t make sense to compare two situations where you are exchanging forecast and actual values, and defend MAPE actually &lt;strong&gt;is symmetric&lt;/strong&gt; because you can’t get a lower MAPE just by lowering your forecasts:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Case
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Actual
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Forecast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
AbsoluteDifference
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
MAPE
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
150
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;thesolution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The…solution?&lt;/h2&gt;
&lt;p&gt;Trying to solve the alledgelly asymmetry, some alternative versions have been proposed. The more general one is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ sMAPE = \frac{200}{N} \sum_{t=1}^{N}\frac{|A_t - F_t|}{|A_t| + |F_t|} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;First thing to notice is that the range of this symmetric MAPE is &lt;span class=&#34;math inline&#34;&gt;\(\big[0,200 \big]\)&lt;/span&gt; which is somewhat antiintuitive. I can’t see myself explaining model deviations in such metric to anybody with a non-technical background.
This could be solved simply dividing by 2, although that would be an aesthetic change only.
On the bright side, this version of sMAPE doesn’t diverge, which brings some sanity and stability to the metric.&lt;/p&gt;
&lt;p&gt;But the aspect that really fascinates me is that the so-called &lt;strong&gt;symmetric MAPE is not symmetric.&lt;/strong&gt;
In fact, sMAPE symmetrizes the asymmetric case above, and the other way around:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exchanging actual and forecast values &lt;em&gt;does&lt;/em&gt; produce the same sMAPE value.&lt;/li&gt;
&lt;/ul&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Case
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Actual
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Forecast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
AbsoluteDifference
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
MAPE
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sMAPE
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
150
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
150
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Modifying the forecast while holding fixed actual values and absolute deviation &lt;em&gt;do not&lt;/em&gt; produce the same sMAPE value. This maybe the most important case.&lt;/li&gt;
&lt;/ul&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Case
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Actual
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Forecast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
AbsoluteDifference
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
MAPE
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sMAPE
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
150
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This second point is a critical issue: simply &lt;strong&gt;biasing the model without improving its accuracy should never produce different error values.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Taking all this into account, the proposed metric seems to be even worse than the original one. Quite surprising!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-alternative&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An alternative&lt;/h2&gt;
&lt;p&gt;Some others have proposed the log ratio &lt;span class=&#34;math inline&#34;&gt;\(ln(F_t/A_t)\)&lt;/span&gt; as a better alternative to MAPE.
It shows, indeed, better statistical properties than others metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Applying least squares regression to this metric estimates the geometric mean whereas minimizing MAPE or sMAPE does not lead to an established measure of location.&lt;/li&gt;
&lt;li&gt;Given that geometric mean cannot exceed arithmetic mean, using least squares with log ratio will be more robust to outliers than OLS.&lt;/li&gt;
&lt;li&gt;Its values belong to a symmetric range: &lt;span class=&#34;math inline&#34;&gt;\([-\infty, \infty]\)&lt;/span&gt;. However, it shows the same asymmetric behaviour than sMAPE: Exchanging actuals and forecasts holds the error
(with a negative value!), but similar absolute deviations with same actual value don’t correspond with equal error values.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If interested in more information about this less-known metric, check out &lt;a href=&#34;https://poseidon01.ssrn.com/delivery.php?ID=912071094074109094069115029093025110000069085067010018086003069122074000022002100121034011107062116011014121092068067002120016015069090069028013066084099122096112019086019005105020067099126068064011082116019102096003091124077118086097090097087121114&amp;amp;EXT=pdf&#34;&gt;A better measure of relative prediction accuracy for model selection and model estimation (Chris Tofallis, 2015)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I hope this may have brought some light into this quirky behaviour of MAPE and sMAPE.&lt;/p&gt;
&lt;p&gt;I also wrote a more general discussion about &lt;a href=&#34;../Energy-forecasting-error-metrics&#34;&gt;forecasting error metrics&lt;/a&gt; you might want to take a look at.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Turn your GGplot to 3D animation. Awesome 2D to 3D plots in R with Rayshader</title>
      <link>/post/ggplot-to-3d-in-r-with-rayshader/</link>
      <pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/post/ggplot-to-3d-in-r-with-rayshader/</guid>
      <description>


&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;In 7 minutes reading, You will learn how to turn your ggplot visualizations into amazing interactive 3D plots you can export or embed in HTML/Rmarkdown.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Or even better, you will export as mp4 an animation rotating the figure.&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img src=&#34;/post/1c-ggplot-to-3d-in-r-with-rayshader/1c-ggplot-to-3d-in-r-with-rayshader_files/rayshader_header.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;1. Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;During the last weeks a ‘new’ package has received the R community attention. We say ‘new’ because it joined recently the CRAN, althought the very first commits in github repo date back more than a year. Its name is &lt;strong&gt;rayshader&lt;/strong&gt; and in the author’s own words:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“rayshader uses elevation data in a base R matrix and a combination of raytracing, spherical texture mapping, overlays, and ambient occlusion to generate beautiful topographic 2D and 3D maps”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In my view, Tyler Morgan-Wall (package’s author) hit the jackpot with the new addition of two specific functions. These are plot_gg() and render_movie(). The first one converts the ggplot to a 3D figure using one or two lines of code making it deadly-simple. The second one renders an animation in which we can set up several parameters like zoom, fps, angles and inclinations… as user-friendly as possible.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/1c-ggplot-to-3d-in-r-with-rayshader/1c-ggplot-to-3d-in-r-with-rayshader_files/rayshader_repo.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;lets-try-these-new-functionalities&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;Let’s try these new functionalities! &lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The only condition you must have a color or fill &lt;em&gt;aesthetic&lt;/em&gt;, unless you can also play in the same plot wiht size.
As we discussed in “XXXXXXX” 3D are not the right choice for most of the data visualization cases. Therefore, I tried to bring to this article a non gratuitous example.&lt;/p&gt;
&lt;p&gt;As a practical challenge, we will visualize in an interactive 3D map the average age in each city of Spain. Cool? First of all we need the population stats. We get it from the INE webpage. Secondly we have to delimiter Spanish cities with they GIS coordinates. Then we are merging these data to create a ggplot chart. Once we have the ggplot object we are going to use the rayshader package to map color aesthetic to the third spatial dimension. To conclude, we are going to render it as rotating 3D video.&lt;/p&gt;
&lt;p&gt;Let´s do it step by step.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;visualazing-spanish-cities-average-age.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;2. Visualazing Spanish cities average age.&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;We usually want to start our pratical work drawing the main steps in our project and our principal goals. So in a general layer, we want to visualiza the average age. Firstly in a ggplot-color way, go one step further and make the plot 3D and end with an animation where the Z axis will be the average age.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;downloading-census-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2.1- Downloading census data&lt;/h3&gt;
&lt;p&gt;As said, for our purpose, we need to collect data from two sources. We use INE open data portal to download census ages data by city. After a not very user-friendly search, we got it. I provide you the following link, where you can find the continuous register statistics:
&lt;a href=&#34;http://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&amp;amp;cid=1254736177012&amp;amp;menu=resultados&amp;amp;secc=1254736195461&amp;amp;idp=1254734710990&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Aiming to keep focused, we don’t get distracted and we are going to download the 2018 file. However, is worth noting the &lt;a href=&#34;https://github.com/oddworldng/INEbaseR/&#34;&gt;&lt;em&gt;INEbase&lt;/em&gt;&lt;/a&gt; efforts to make easier the INE open data platform.&lt;/p&gt;
&lt;p&gt;We start loading (or downloading) the packages we are going to use. In other article or tip we will provide a custom function to Load and Download Rpackages in onle line.
Moreover we define the required functions and download directories.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if (!require(pxR)) {
  install.packages(&amp;quot;pxR&amp;quot;, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  require(pxR)
}
if (!require(RColorBrewer)) {
  install.packages(&amp;quot;RColorBrewer&amp;quot;, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  require(RColorBrewer)
}

if (!require(rgeos)) {
  install.packages(&amp;quot;rgeos&amp;quot;, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  require(rgeos)
  install.packages(&amp;quot;rgdal&amp;quot;, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;) # reinstall cause gpclib dependencie https://stackoverflow.com/questions/30790036/error-istruegpclibpermitstatus-is-not-true
  require(rgdal)
}
if (!require(rgdal)) {
  install.packages(&amp;quot;rgdal&amp;quot;, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  require(rgdal)
}
if (!require(knitr)) {
  install.packages(&amp;quot;knitr&amp;quot;, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  require(knitr)
}
if (!require(rayshader)) {
  install.packages(&amp;quot;rayshader&amp;quot;, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  require(rayshader)
}
if (!require(magrittr)) {
  install.packages(&amp;quot;magrittr&amp;quot;, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  require(magrittr)
}
if (!require(tidyverse)) {
  install.packages(&amp;quot;tidyverse&amp;quot;, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  require(tidyverse)
}
if (!require(RColorBrewer)) {
  install.packages(&amp;quot;RColorBrewer&amp;quot;, repos = &amp;quot;http://cran.us.r-project.org&amp;quot;)
  require(RColorBrewer)
}
as.numeric.factor &amp;lt;- function(x) {    # Custom function to convert fctr to num factor value
    return(suppressWarnings(as.numeric(levels(x))[x]))
}
 
if(!dir.exists(&amp;quot;data&amp;quot;)) dir.create(&amp;quot;data&amp;quot;)  # Create the download directory&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Downloading INE 2018 file:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;utils::download.file(url = &amp;quot;http://www.ine.es/pcaxisdl/t20/e245/p05/a2018/l0/00000006.px&amp;quot;,
                     destfile = &amp;quot;data/census_2018.px&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).
## This warning is displayed once per session.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tbl_census_2018 &amp;lt;- read.px(&amp;quot;data/census_2018.px&amp;quot;) %&amp;gt;%              # Load &amp;amp; format
  as.tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We parse the data to obtain a name,pcode,average age dataframe&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tbl_census_2018 %&amp;lt;&amp;gt;% 
  set_names(c(&amp;quot;age&amp;quot;, &amp;quot;city&amp;quot;, &amp;quot;sex&amp;quot;, &amp;quot;population&amp;quot;)) %&amp;gt;%             # Cambiamos los nombre
  na.omit() %&amp;gt;%                                                    # Na rmv
  filter((city!=&amp;quot;Total&amp;quot;)&amp;amp;(age!=&amp;quot;Total&amp;quot;)&amp;amp;(sex==&amp;quot;Ambos sexos&amp;quot;)) %&amp;gt;%  # Duplicate info rmv
  separate(city, c(&amp;#39;postal_code&amp;#39;, &amp;#39;city_name&amp;#39;), sep=&amp;quot;-&amp;quot;) %&amp;gt;%       # Sep City column
  mutate(age = as.numeric.factor(age)) %&amp;gt;%                         # Conv to numeric
  group_by(city_name, postal_code) %&amp;gt;%                             # Group to operate
  summarise(avg_age = sum(population*age,na.rm = T)/sum(population,na.rm=T)) %&amp;gt;%  # Avg age
  select(city_name, postal_code, avg_age)                          # Discard columns
kable(                                                             # Just Rmarkdown format
  tbl_census_2018 %&amp;gt;% head(2)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;city_name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;postal_code&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;avg_age&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Ababuj&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;44001&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;52.40789&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Abades&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;40001&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45.40000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;downloading-gis-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2.2- Downloading GIS data&lt;/h3&gt;
&lt;p&gt;The second source is the Geo data. We will use cities coordinates and matching it with Spanish register data previously obtained.&lt;/p&gt;
&lt;p&gt;Downloading map overlay:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp &amp;lt;- tempfile()                              # Create the tempfile
u=&amp;quot;http://www.arcgis.com/sharing/rest/content/items/8e31c4c1a0b348f79058f212d0d807a1/data&amp;quot;
utils::download.file(url = u, destfile = temp,
                     mode=&amp;quot;wb&amp;quot;)                 # Binary mode for correct download

unzip(temp, exdir = &amp;quot;data/cities_gis&amp;quot;)          # Unzip in data/cities_gis
unlink(temp)                                    # Delete temp file&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We parse the spatial information to convert it into tabular data. We expect that the Canary Islands coordinates will skew the plot, so it’s our decision to keep focused in our 3D objetive and filter peninsular coordinates. It’s also possible, and a better practice, to move insular coordinates looking for a compact plot, instead of filter them out.&lt;/p&gt;
&lt;p&gt;To complete this data processing, we use fortify function that allows us to don’t load more packages. However, this function throws a warning suggesting the broom::tidy() one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tlb_cities_gis &amp;lt;- readOGR(dsn = &amp;quot;data/cities_gis/Municipios_ETRS89_30N.shp&amp;quot;,
                              verbose=FALSE) # Spatial data reading&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tlb_cities_gis %&amp;lt;&amp;gt;% 
  fortify(region = &amp;quot;Codigo&amp;quot;) # %&amp;gt;%             # Conv &amp;quot;spatial object&amp;quot; to data.frame
  # broom::tidy()

plot_canarias &amp;lt;- F                              # Control param, initial app config

if(plot_canarias==F){                           # Should be moduled in a funct
  tlb_cities_gis %&amp;lt;&amp;gt;%
  filter((long&amp;gt;0) &amp;amp; (lat&amp;gt;4000000))              # Filter peninsular data
} &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finaly, we join both creating the final dataset, which we are going to use to make the plots. Note that we use left join to keep de geo data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tbl_cities_avg_age &amp;lt;- tlb_cities_gis %&amp;gt;% 
  left_join(tbl_census_2018, by = c(&amp;quot;id&amp;quot; = &amp;quot;postal_code&amp;quot;)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a good practice, we are going to check the number of NAs generated after the left join. These NAs meaning is that there are cities localized but without average year information&lt;/p&gt;
&lt;p&gt;We can see that these missing values represents just 1% of the data, so we are going to impute them with the previous postal code info. I bet that you can easily improve this procedure but I consider it’s prety acceptable enought seeing the low NA ratio.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(                                                      # RMarkdown output format
  tbl_cities_avg_age %&amp;gt;%
    group_by(id) %&amp;gt;%
    summarise(na = sum(is.na(avg_age))) %&amp;gt;%                 # NAs by city
    summarise(missing_perc = sum(na&amp;gt;0)/length(na)*100) %&amp;gt;%  # Perc cities with at least 1 na 
    select(missing_perc)
  ,
  align = &amp;quot;c&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;missing_perc&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0.9268413&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tbl_cities_avg_age %&amp;lt;&amp;gt;%       
  arrange(id) %&amp;gt;% 
  fill(avg_age, .direction = &amp;quot;down&amp;quot;)            # Fill with the previous pc data.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ggplot-visualization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2.3- GGplot visualization&lt;/h3&gt;
&lt;p&gt;Inspired in &lt;a href=&#34;http://blog.manugarri.com/making-a-beautiful-map-of-spain-in-ggplot2/&#34; class=&#34;uri&#34;&gt;http://blog.manugarri.com/making-a-beautiful-map-of-spain-in-ggplot2/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Once we have created the final dataset, we are able to start ploting it. Of course longitude in X-axis and latitude en Y-axis. Firstly average city age is represented using a color palette. Red colours are assigned to older people and blue ones to younger city population. We get it in ggplot with the fill aesthetic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myPalette &amp;lt;- colorRampPalette(rev(brewer.pal(11, &amp;quot;Spectral&amp;quot;)))      # Create reverse Spectral palette

plot_cities &amp;lt;- ggplot() +
  geom_polygon(data = tbl_cities_avg_age, aes(fill = avg_age,   
                                         x = long, 
                                         y = lat, 
                                         group = id)) +      # Dummy variable to correct fill by PCode.
  scale_fill_gradientn(colours=myPalette(4)) +                 # Choose palette colours.
  labs(fill=&amp;quot;Avg age&amp;quot;)
plot(plot_cities)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/ggplot-to-3d-in-r-with-rayshader_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;d-rayshader-visualization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2.4- 3D Rayshader Visualization!&lt;/h3&gt;
&lt;p&gt;That was pretty nice. It’s sure that you can reach the general propose to be able to locate inmediately older an younger zones. Although as we discuss in XXX, human eyes aren’t ready to distinguiss almost nothing but big color contrasts. What about complement color with a third dimension through z axis?&lt;/p&gt;
&lt;p&gt;Let’s see how it works&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_gg(plot_cities,multicore=TRUE,width=5,height=3,scale=310)    # Plot_gg de rayshader
render_snapshot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/ggplot-to-3d-in-r-with-rayshader_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;200%&#34; height=&#34;200%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Hmm you told something about render_movie()… What if we anime it?&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;d-animation-with-rayshader&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2.5- 3D animation with rayshader&lt;/h3&gt;
&lt;p&gt;In the last plot, it results the correct angle election as a key point. But what if we animate it with a rotating effect?&lt;/p&gt;
&lt;p&gt;This is what the following function take cares on:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;render_movie(&amp;quot;img/movie_spain.mp4&amp;quot;,frames = 720, fps=30,zoom=0.6,fov = 30)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/1c-ggplot-to-3d-in-r-with-rayshader/1c-ggplot-to-3d-in-r-with-rayshader_files/movie_spain.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
